{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daveAnalyst/zindi-mhp-energy-prediction-2025/blob/master/zindi_mhp_dev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "562fd2ac",
      "metadata": {
        "id": "562fd2ac"
      },
      "source": [
        "# Zindi Micro-Hydropower Energy Load Prediction\n",
        "\n",
        "This notebook aims to predict daily energy consumption (kWh) per data user for Micro-Hydropower Plants (MHPs) in Kalam, Pakistan. We will use MHP sensor data and climate indicators to build a predictive model.\n",
        "\n",
        "**Objective:** Forecast total daily kWh per user for one month into the future.\n",
        "**Metric:** Root Mean Squared Error (RMSE)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1A: Setup and Git Sync (Corrected Data Setup)\n",
        "\n",
        "import os\n",
        "import time\n",
        "from google.colab import drive\n",
        "import zipfile # Make sure zipfile is imported\n",
        "import shutil # Make sure shutil is imported\n",
        "\n",
        "print(\"--- Starting Day 3 Setup ---\")\n",
        "setup_start_time = time.time()\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"Drive mounted.\")\n",
        "\n",
        "# 2. Define Repo Name and Path\n",
        "repo_name = \"zindi-mhp-energy-prediction-2025\"\n",
        "base_content_path = '/content'\n",
        "repo_path = os.path.join(base_content_path, repo_name)\n",
        "print(f\"Target repository path: {repo_path}\")\n",
        "\n",
        "# 3. Clone or Change Directory, Configure Git User\n",
        "print(\"\\nCloning repository if needed...\")\n",
        "if not os.path.exists(repo_path):\n",
        "    # --- IMPORTANT: Replace with your NEW PAT ---\n",
        "    PAT = \"ghp_YOUR_NEW_PAT_HERE\" # Replace with the PAT you generated today\n",
        "    # ---------------------------------------\n",
        "    clone_url = f\"https://{PAT}@github.com/daveAnalyst/{repo_name}.git\"\n",
        "    !git clone {clone_url}\n",
        "    if not os.path.exists(repo_path):\n",
        "         print(\"--- ERROR: Clone failed! Cannot proceed. ---\")\n",
        "         raise RuntimeError(\"Git clone failed\")\n",
        "    print(\"Repository cloned.\")\n",
        "    os.chdir(repo_path)\n",
        "else:\n",
        "    print(\"Repository already exists.\")\n",
        "    if os.path.basename(os.getcwd()) != repo_name:\n",
        "        os.chdir(repo_path)\n",
        "print(f\"Current directory: {os.getcwd()}\")\n",
        "\n",
        "print(\"\\nConfiguring Git user...\")\n",
        "GIT_EMAIL = \"your_github_email@example.com\" #<-- YOUR GITHUB EMAIL\n",
        "GIT_NAME = \"Your GitHub Name\" #<-- YOUR GITHUB NAME/USERNAME\n",
        "!git config --global user.email \"{GIT_EMAIL}\"\n",
        "!git config --global user.name \"{GIT_NAME}\"\n",
        "print(\"Git user configured.\")\n",
        "\n",
        "# 4. Pull Latest Changes from GitHub\n",
        "print(\"\\nPulling latest changes from origin/master...\")\n",
        "!git pull origin master\n",
        "print(\"Pull complete.\")\n",
        "\n",
        "# 5. Install Required Packages\n",
        "print(\"\\nInstalling/Updating required packages...\")\n",
        "!pip install optuna lightgbm==4.1.0 openpyxl pandas numpy scikit-learn --quiet\n",
        "print(\"Packages installed/updated.\")\n",
        "\n",
        "# 6. Unzip/Copy Data from Drive to Colab Runtime (Corrected Logic)\n",
        "DRIVE_DATA_PATH = '/content/drive/MyDrive/Zindi MHP Challenge/data' # Adjust if needed\n",
        "COLAB_DATA_PATH = '/content/data'\n",
        "DATA_DIR = COLAB_DATA_PATH\n",
        "\n",
        "print(f\"\\nSetting up data in '{DATA_DIR}' from '{DRIVE_DATA_PATH}'...\")\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# --- Unzip Data.csv ---\n",
        "print(\"  Unzipping Data.zip...\")\n",
        "try:\n",
        "    with zipfile.ZipFile(os.path.join(DRIVE_DATA_PATH, 'Data.zip'), 'r') as z: z.extractall(DATA_DIR)\n",
        "    print(\"    Data.zip unzipped.\")\n",
        "except FileNotFoundError: print(f\"    ERROR: Data.zip not found at {os.path.join(DRIVE_DATA_PATH, 'Data.zip')}\")\n",
        "except Exception as e: print(f\"    ERROR unzipping Data.zip: {e}\")\n",
        "\n",
        "# --- Unzip Climate Data.xlsx --- <<< CORRECTION HERE >>>\n",
        "print(\"  Unzipping Climate Data.zip...\")\n",
        "climate_zip_path = os.path.join(DRIVE_DATA_PATH, 'Climate Data.zip')\n",
        "excel_final_name = 'Kalam Climate Data.xlsx' # The target filename\n",
        "excel_path_in_colab = os.path.join(DATA_DIR, excel_final_name)\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(climate_zip_path, 'r') as zip_ref:\n",
        "        # Extract all contents first (might create subfolder)\n",
        "        zip_ref.extractall(DATA_DIR)\n",
        "        print(f\"    Climate Data.zip extracted to {DATA_DIR}. Checking contents...\")\n",
        "        # Try to find the Excel file, potentially in a subfolder named like the zip\n",
        "        extracted_excel_path = None\n",
        "        possible_subfolder_name = 'Climate Data' # Common pattern\n",
        "        path_in_subfolder = os.path.join(DATA_DIR, possible_subfolder_name, excel_final_name)\n",
        "        path_direct = os.path.join(DATA_DIR, excel_final_name)\n",
        "\n",
        "        if os.path.exists(path_in_subfolder):\n",
        "             print(f\"    Found Excel file in subfolder: {path_in_subfolder}\")\n",
        "             # Move it out of the subfolder\n",
        "             print(f\"    Moving Excel file to {excel_path_in_colab}\")\n",
        "             shutil.move(path_in_subfolder, excel_path_in_colab)\n",
        "             # Optionally remove the now empty subfolder and __MACOSX if it exists\n",
        "             if os.path.exists(os.path.join(DATA_DIR, possible_subfolder_name)): shutil.rmtree(os.path.join(DATA_DIR, possible_subfolder_name))\n",
        "             if os.path.exists(os.path.join(DATA_DIR, '__MACOSX')): shutil.rmtree(os.path.join(DATA_DIR, '__MACOSX'))\n",
        "             extracted_excel_path = excel_path_in_colab\n",
        "        elif os.path.exists(path_direct):\n",
        "             print(f\"    Found Excel file directly: {path_direct}\")\n",
        "             extracted_excel_path = path_direct\n",
        "        else:\n",
        "             print(f\"    ERROR: Cannot find '{excel_final_name}' after extracting zip.\")\n",
        "             print(f\"    Contents of {DATA_DIR}:\")\n",
        "             !ls -R \"{DATA_DIR}\" # Show contents to help debug\n",
        "\n",
        "    if extracted_excel_path and os.path.exists(extracted_excel_path):\n",
        "        print(f\"    Climate data Excel file is ready at: {extracted_excel_path}\")\n",
        "    else:\n",
        "        print(f\"    ERROR: Problem setting up climate data Excel file.\")\n",
        "\n",
        "except FileNotFoundError: print(f\"    ERROR: Climate Data.zip not found at {climate_zip_path}\")\n",
        "except Exception as e: print(f\"    ERROR unzipping Climate Data.zip: {e}\")\n",
        "\n",
        "\n",
        "# --- Copy SampleSubmission.csv ---\n",
        "print(\"  Copying SampleSubmission.csv...\")\n",
        "try:\n",
        "    shutil.copy(os.path.join(DRIVE_DATA_PATH, 'SampleSubmission.csv'), DATA_DIR)\n",
        "    print(\"    SampleSubmission.csv copied.\")\n",
        "except FileNotFoundError: print(f\"    ERROR: SampleSubmission.csv not found at {os.path.join(DRIVE_DATA_PATH, 'SampleSubmission.csv')}\")\n",
        "except Exception as e: print(f\"    ERROR copying SampleSubmission.csv: {e}\")\n",
        "\n",
        "\n",
        "print(\"\\nFinal check of Colab data directory:\")\n",
        "!ls -lh \"{DATA_DIR}\"\n",
        "\n",
        "setup_time = time.time() - setup_start_time\n",
        "print(f\"\\n--- Setup finished in {setup_time:.1f}s ---\")\n",
        "print(f\"\\nIMPORTANT: Ensure DATA_DIR for subsequent cells is: {DATA_DIR}\")"
      ],
      "metadata": {
        "id": "y_PALnKIZjqU",
        "outputId": "cb09a95c-252c-4ab7-c4d3-64e41e060b15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "id": "y_PALnKIZjqU",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Day 3 Setup ---\n",
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive mounted.\n",
            "Target repository path: /content/zindi-mhp-energy-prediction-2025\n",
            "\n",
            "Cloning repository if needed...\n",
            "Repository already exists.\n",
            "Current directory: /content/zindi-mhp-energy-prediction-2025\n",
            "\n",
            "Configuring Git user...\n",
            "Git user configured.\n",
            "\n",
            "Pulling latest changes from origin/master...\n",
            "From https://github.com/daveAnalyst/zindi-mhp-energy-prediction-2025\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n",
            "Pull complete.\n",
            "\n",
            "Installing/Updating required packages...\n",
            "Packages installed/updated.\n",
            "\n",
            "Setting up data in '/content/data' from '/content/drive/MyDrive/Zindi MHP Challenge/data'...\n",
            "  Unzipping Data.zip...\n",
            "    Data.zip unzipped.\n",
            "  Unzipping Climate Data.zip...\n",
            "    Climate Data.zip extracted to /content/data. Checking contents...\n",
            "    Found Excel file in subfolder: /content/data/Climate Data/Kalam Climate Data.xlsx\n",
            "    Moving Excel file to /content/data/Kalam Climate Data.xlsx\n",
            "    Climate data Excel file is ready at: /content/data/Kalam Climate Data.xlsx\n",
            "  Copying SampleSubmission.csv...\n",
            "    SampleSubmission.csv copied.\n",
            "\n",
            "Final check of Colab data directory:\n",
            "total 2.8G\n",
            "-rw-r--r-- 1 root root 2.8G Apr  6 12:25  Data.csv\n",
            "-rw-r--r-- 1 root root 735K Apr  6 12:25 'Kalam Climate Data.xlsx'\n",
            "-rw------- 1 root root 260K Apr  6 12:25  SampleSubmission.csv\n",
            "\n",
            "--- Setup finished in 35.1s ---\n",
            "\n",
            "IMPORTANT: Ensure DATA_DIR for subsequent cells is: /content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "60f49b94",
      "metadata": {
        "id": "60f49b94",
        "outputId": "5a3cf659-965f-4931-b420-7b85344aba87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported and settings configured.\n",
            "Data directory set to: data\n",
            "Available plotting styles: ['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'petroff10', 'seaborn-v0_8', 'seaborn-v0_8-bright', 'seaborn-v0_8-colorblind', 'seaborn-v0_8-dark', 'seaborn-v0_8-dark-palette', 'seaborn-v0_8-darkgrid', 'seaborn-v0_8-deep', 'seaborn-v0_8-muted', 'seaborn-v0_8-notebook', 'seaborn-v0_8-paper', 'seaborn-v0_8-pastel', 'seaborn-v0_8-poster', 'seaborn-v0_8-talk', 'seaborn-v0_8-ticks', 'seaborn-v0_8-white', 'seaborn-v0_8-whitegrid', 'tableau-colorblind10']\n"
          ]
        }
      ],
      "source": [
        "# Basic libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math # For sqrt\n",
        "import os\n",
        "\n",
        "# Modeling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Settings\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "DATA_DIR = 'data' # Set the path to your data directory\n",
        "\n",
        "# Ensure plots are displayed inline and set a style\n",
        "%matplotlib inline\n",
        "# Use a style that's likely available - adjust if needed\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "# If the above fails, try: plt.style.use('seaborn-darkgrid') or plt.style.use('ggplot')\n",
        "\n",
        "print(\"Libraries imported and settings configured.\")\n",
        "print(f\"Data directory set to: {DATA_DIR}\")\n",
        "print(f\"Available plotting styles: {plt.style.available}\") # Optional: see available styles"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# ---- IMPORTANT: SET THIS CORRECTLY ----\n",
        "# Option 1: If you unzipped/copied data to Colab runtime storage\n",
        "DATA_DIR = '/content/data'\n",
        "# Option 2: If you are reading directly from Google Drive (potentially slower)\n",
        "# DATA_DIR = '/content/drive/MyDrive/Zindi MHP Challenge/data' # Adjust path if needed\n",
        "# --------------------------------------\n",
        "\n",
        "print(f\"Using DATA_DIR: {DATA_DIR}\")\n",
        "\n",
        "# Check 1: Does DATA_DIR exist?\n",
        "print(f\"\\nChecking existence of DATA_DIR...\")\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    print(f\"--- ERROR ---: Directory '{DATA_DIR}' does NOT exist!\")\n",
        "    print(\"Action: Ensure you have run the Drive mount AND the data unzipping/copying cell correctly in this session.\")\n",
        "else:\n",
        "    print(f\"Directory '{DATA_DIR}' exists.\")\n",
        "\n",
        "    # Check 2: List files in DATA_DIR (Check names and capitalization!)\n",
        "    print(f\"\\nFiles present in '{DATA_DIR}':\")\n",
        "    !ls -lh \"{DATA_DIR}\" # Use shell command for clear listing\n",
        "\n",
        "    # Check 3: Verify specific expected files\n",
        "    expected_files = ['Data.csv', 'Kalam Climate Data.xlsx', 'SampleSubmission.csv']\n",
        "    print(\"\\nChecking for expected files:\")\n",
        "    all_found = True\n",
        "    for f_name in expected_files:\n",
        "        f_path = os.path.join(DATA_DIR, f_name)\n",
        "        if os.path.exists(f_path):\n",
        "            print(f\"  [ OK ] Found: {f_name}\")\n",
        "        else:\n",
        "            print(f\"  [FAIL] MISSING: {f_name} at path {f_path}\")\n",
        "            all_found = False\n",
        "    if not all_found:\n",
        "        print(\"--- ERROR ---: One or more required data files are missing from DATA_DIR.\")\n",
        "        print(\"Action: Re-run the data unzipping/copying cell, checking its output for errors.\")\n",
        "    else:\n",
        "        print(\"All expected files seem to be present.\")\n",
        "\n",
        "# Check 4: Ensure openpyxl is installed (relevant for Excel)\n",
        "print(\"\\nChecking for openpyxl installation...\")\n",
        "try:\n",
        "    import openpyxl\n",
        "    print(\"  [ OK ] openpyxl is installed.\")\n",
        "except ImportError:\n",
        "    print(\"  [FAIL] openpyxl is NOT installed.\")\n",
        "    print(\"Action: Run '!pip install openpyxl' in a cell.\")"
      ],
      "metadata": {
        "id": "bi7Syey5eF5a",
        "outputId": "a30c3bb0-59e9-4fef-ad2d-93efa88c9e2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "bi7Syey5eF5a",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using DATA_DIR: /content/data\n",
            "\n",
            "Checking existence of DATA_DIR...\n",
            "Directory '/content/data' exists.\n",
            "\n",
            "Files present in '/content/data':\n",
            "total 2.8G\n",
            "-rw-r--r-- 1 root root 2.8G Apr  6 12:25  Data.csv\n",
            "-rw-r--r-- 1 root root 735K Apr  6 12:25 'Kalam Climate Data.xlsx'\n",
            "-rw------- 1 root root 260K Apr  6 12:25  SampleSubmission.csv\n",
            "\n",
            "Checking for expected files:\n",
            "  [ OK ] Found: Data.csv\n",
            "  [ OK ] Found: Kalam Climate Data.xlsx\n",
            "  [ OK ] Found: SampleSubmission.csv\n",
            "All expected files seem to be present.\n",
            "\n",
            "Checking for openpyxl installation...\n",
            "  [ OK ] openpyxl is installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# --- Use the SAME DATA_DIR as verified in Step 1 ---\n",
        "DATA_DIR = '/content/data' # Or your Drive path\n",
        "# ----------------------------------------------------\n",
        "\n",
        "print(\"--- Verifying Column Names ---\")\n",
        "\n",
        "# Check CSV Columns\n",
        "try:\n",
        "    print(\"\\nChecking Data.csv headers...\")\n",
        "    csv_path = os.path.join(DATA_DIR, 'Data.csv')\n",
        "    # Load only 5 rows, use engine='python' if C engine failed before\n",
        "    csv_head = pd.read_csv(csv_path, nrows=5, engine='python')\n",
        "    print(f\"Columns found in Data.csv: {csv_head.columns.tolist()}\")\n",
        "    # --->>> CONFIRM: Are 'date_time' and 'Source' exactly as expected? <<<---\n",
        "except Exception as e:\n",
        "    print(f\"ERROR reading Data.csv header: {e}\")\n",
        "\n",
        "# Check Excel Columns\n",
        "try:\n",
        "    print(\"\\nChecking Kalam Climate Data.xlsx headers...\")\n",
        "    excel_path = os.path.join(DATA_DIR, 'Kalam Climate Data.xlsx')\n",
        "    excel_head = pd.read_excel(excel_path, nrows=5) # Reads first sheet by default\n",
        "    print(f\"Columns found in Kalam Climate Data.xlsx: {excel_head.columns.tolist()}\")\n",
        "    # --->>> CONFIRM: Is 'Date_Time' exactly as expected? <<<---\n",
        "    # Let's also see the format of the date column\n",
        "    date_col_name = 'Date_Time' # Use the actual column name found\n",
        "    if date_col_name in excel_head.columns:\n",
        "         print(f\"Sample values in Excel column '{date_col_name}':\")\n",
        "         print(excel_head[date_col_name].head())\n",
        "    else:\n",
        "         print(f\"Warning: Column '{date_col_name}' not found in Excel header check.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERROR reading Kalam Climate Data.xlsx header: {e}\")"
      ],
      "metadata": {
        "id": "hOcFuzH1ebLQ",
        "outputId": "79e66cac-7399-44d1-cff3-c14ba8b16019",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "hOcFuzH1ebLQ",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Verifying Column Names ---\n",
            "\n",
            "Checking Data.csv headers...\n",
            "Columns found in Data.csv: ['date_time', 'v_red', 'current', 'power_factor', 'kwh', 'Source', 'v_blue', 'v_yellow', 'consumer_device_9', 'consumer_device_x']\n",
            "\n",
            "Checking Kalam Climate Data.xlsx headers...\n",
            "Columns found in Kalam Climate Data.xlsx: ['Date Time', 'Temperature (°C)', 'Dewpoint Temperature (°C)', 'U Wind Component (m/s)', 'V Wind Component (m/s)', 'Total Precipitation (mm)', 'Snowfall (mm)', 'Snow Cover (%)']\n",
            "Warning: Column 'Date_Time' not found in Excel header check.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c02b6671",
      "metadata": {
        "id": "c02b6671"
      },
      "source": [
        "## 2. Load Data\n",
        "\n",
        "Load the datasets:\n",
        "1.  **MHP Data:** `Data.csv` contains the 5-minute interval sensor readings (voltage, current, kWh, etc.). We anticipate the timestamp column is named `date_time`.\n",
        "2.  **Climate Data:** `Kalam Climate Data.xlsx` contains climate indicators (temperature, precipitation, etc.). This is an Excel file.\n",
        "3.  **Sample Submission:** `SampleSubmission.csv` defines the required prediction format and IDs for the test set.\n",
        "\n",
        "*Note: Reading Excel files requires the `openpyxl` library. Install it if needed (`pip install openpyxl`). Reading the MHP CSV might require `engine='python'` if the default C engine fails.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "46194803",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46194803",
        "outputId": "7ce636d2-f456-41ce-af64-45466bbae6a7",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data with verified column names...\n",
            "\n",
            "[0.0s] Attempting to load MHP structure from: /content/data/Data.csv\n",
            "Loading only columns: ['date_time', 'Source', 'kwh']\n",
            "[60.6s] Structure loaded for Data.csv. Size: (39147559, 3)\n",
            "[60.6s] Converting MHP date column ('date_time')...\n",
            "[77.1s] MHP date converted.\n",
            "[77.1s] Loaded and initially processed: Data.csv\n",
            "\n",
            "[77.1s] Attempting to load Excel structure from: /content/data/Kalam Climate Data.xlsx\n",
            "[80.1s] Structure loaded for Kalam Climate Data.xlsx. Size: (12228, 8)\n",
            "Original Excel columns: ['Date Time', 'Temperature (°C)', 'Dewpoint Temperature (°C)', 'U Wind Component (m/s)', 'V Wind Component (m/s)', 'Total Precipitation (mm)', 'Snowfall (mm)', 'Snow Cover (%)']\n",
            "[80.1s] Renaming Excel columns...\n",
            "Columns after renaming: ['date_time_excel', 'temperature', 'dew_point', 'u_wind', 'v_wind', 'precipitation', 'Snowfall (mm)', 'Snow Cover (%)']\n",
            "[80.1s] Calculating wind speed magnitude...\n",
            "Wind speed calculated.\n",
            "[80.1s] Converting Excel date column ('date_time_excel')...\n",
            "[80.3s] Excel date converted.\n",
            "[80.3s] Loaded and initially processed: Kalam Climate Data.xlsx\n",
            "\n",
            "[80.3s] Attempting to load sample submission from: /content/data/SampleSubmission.csv\n",
            "[80.4s] Successfully loaded: SampleSubmission.csv\n",
            "\n",
            "[80.4s] All data loaded successfully.\n",
            "\n",
            "--- MHP Data Info (Post-Processing) ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 39147559 entries, 0 to 39147558\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Dtype         \n",
            "---  ------     -----         \n",
            " 0   timestamp  datetime64[ns]\n",
            " 1   user_id    object        \n",
            " 2   kwh        float64       \n",
            "dtypes: datetime64[ns](1), float64(1), object(1)\n",
            "memory usage: 896.0+ MB\n",
            "\n",
            "--- Climate Data Info (Post-Processing) ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 12228 entries, 0 to 12227\n",
            "Data columns (total 5 columns):\n",
            " #   Column         Non-Null Count  Dtype         \n",
            "---  ------         --------------  -----         \n",
            " 0   timestamp      12228 non-null  datetime64[ns]\n",
            " 1   temperature    12228 non-null  float64       \n",
            " 2   dew_point      12228 non-null  float64       \n",
            " 3   wind_speed     12228 non-null  float64       \n",
            " 4   precipitation  12228 non-null  float64       \n",
            "dtypes: datetime64[ns](1), float64(4)\n",
            "memory usage: 477.8 KB\n",
            "\n",
            "--- Sample Submission Head ---\n",
            "                                          ID  kwh\n",
            "0  2024-09-24_consumer_device_12_data_user_1    0\n",
            "1  2024-09-25_consumer_device_12_data_user_1    0\n",
            "2  2024-09-26_consumer_device_12_data_user_1    0\n",
            "3  2024-09-27_consumer_device_12_data_user_1    0\n",
            "4  2024-09-28_consumer_device_12_data_user_1    0\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Load Data (Revised with Correct Column Names)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np # Make sure numpy is imported\n",
        "import os\n",
        "import time\n",
        "\n",
        "# --- Use the SAME DATA_DIR as verified previously ---\n",
        "# Ensure this is set correctly for your Colab environment\n",
        "DATA_DIR = '/content/data' # Or your Drive path: '/content/drive/MyDrive/Zindi MHP Challenge/data'\n",
        "# ----------------------------------------------------\n",
        "\n",
        "print(\"Loading data with verified column names...\")\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    # --- Load MHP Data (CSV) ---\n",
        "    mhp_filename = 'Data.csv'\n",
        "    mhp_path = os.path.join(DATA_DIR, mhp_filename)\n",
        "    print(f\"\\n[{time.time() - start_time:.1f}s] Attempting to load MHP structure from: {mhp_path}\")\n",
        "\n",
        "    # Columns verified as correct in Data.csv\n",
        "    mhp_cols_to_load = ['date_time', 'Source', 'kwh']\n",
        "    print(f\"Loading only columns: {mhp_cols_to_load}\")\n",
        "\n",
        "    mhp_data_raw = pd.read_csv(\n",
        "        mhp_path,\n",
        "        usecols=mhp_cols_to_load,\n",
        "        #engine='python'  Keep if C engine failed\n",
        "    )\n",
        "    print(f\"[{time.time() - start_time:.1f}s] Structure loaded for {mhp_filename}. Size: {mhp_data_raw.shape}\")\n",
        "\n",
        "    # Convert date column AFTER loading\n",
        "    print(f\"[{time.time() - start_time:.1f}s] Converting MHP date column ('date_time')...\")\n",
        "    mhp_data_raw['timestamp'] = pd.to_datetime(mhp_data_raw['date_time'], errors='coerce')\n",
        "    if mhp_data_raw['timestamp'].isnull().any():\n",
        "        print(\"Warning: Some MHP date conversions failed (resulted in NaT).\")\n",
        "    mhp_data_raw.drop(columns=['date_time'], inplace=True) # Drop original string column\n",
        "    print(f\"[{time.time() - start_time:.1f}s] MHP date converted.\")\n",
        "\n",
        "    # Rename user ID column (verified as 'Source')\n",
        "    mhp_data_raw.rename(columns={'Source': 'user_id'}, inplace=True)\n",
        "    print(f\"[{time.time() - start_time:.1f}s] Loaded and initially processed: {mhp_filename}\")\n",
        "\n",
        "\n",
        "    # --- Load Climate Data (Excel) ---\n",
        "    climate_filename = 'Kalam Climate Data.xlsx'\n",
        "    climate_path = os.path.join(DATA_DIR, climate_filename)\n",
        "    print(f\"\\n[{time.time() - start_time:.1f}s] Attempting to load Excel structure from: {climate_path}\")\n",
        "    # Load full Excel structure first, do not parse dates yet\n",
        "    climate_data_raw = pd.read_excel(climate_path)\n",
        "    print(f\"[{time.time() - start_time:.1f}s] Structure loaded for {climate_filename}. Size: {climate_data_raw.shape}\")\n",
        "    print(f\"Original Excel columns: {climate_data_raw.columns.tolist()}\") # Show original names\n",
        "\n",
        "    # --- Rename Excel Columns to Simpler Names ---\n",
        "    # Use the EXACT names found in the verification step\n",
        "    print(f\"[{time.time() - start_time:.1f}s] Renaming Excel columns...\")\n",
        "    climate_rename_map = {\n",
        "        'Date Time': 'date_time_excel', # Temporary name for date\n",
        "        'Temperature (°C)': 'temperature',\n",
        "        'Dewpoint Temperature (°C)': 'dew_point',\n",
        "        'U Wind Component (m/s)': 'u_wind',\n",
        "        'V Wind Component (m/s)': 'v_wind',\n",
        "        'Total Precipitation (mm)': 'precipitation'\n",
        "        # Add other columns here if needed later, e.g., 'Snowfall (mm)': 'snowfall'\n",
        "    }\n",
        "    # Check if all expected columns exist before renaming\n",
        "    missing_cols = [col for col in climate_rename_map.keys() if col not in climate_data_raw.columns]\n",
        "    if missing_cols:\n",
        "         raise KeyError(f\"Missing expected columns in Excel file: {missing_cols}\")\n",
        "\n",
        "    climate_data_raw.rename(columns=climate_rename_map, inplace=True)\n",
        "    print(f\"Columns after renaming: {climate_data_raw.columns.tolist()}\")\n",
        "\n",
        "    # --- Calculate Wind Speed Magnitude ---\n",
        "    print(f\"[{time.time() - start_time:.1f}s] Calculating wind speed magnitude...\")\n",
        "    climate_data_raw['wind_speed'] = np.sqrt(climate_data_raw['u_wind']**2 + climate_data_raw['v_wind']**2)\n",
        "    # Drop original U/V components if no longer needed\n",
        "    # climate_data_raw.drop(columns=['u_wind', 'v_wind'], inplace=True)\n",
        "    print(f\"Wind speed calculated.\")\n",
        "\n",
        "    # --- Convert Date Column AFTER Loading and Renaming ---\n",
        "    print(f\"[{time.time() - start_time:.1f}s] Converting Excel date column ('date_time_excel')...\")\n",
        "    climate_data_raw['timestamp'] = pd.to_datetime(climate_data_raw['date_time_excel'], errors='coerce')\n",
        "    if climate_data_raw['timestamp'].isnull().any():\n",
        "        print(\"Warning: Some Excel date conversions failed (resulted in NaT).\")\n",
        "    # Drop original date column AFTER successful conversion\n",
        "    climate_data_raw.drop(columns=['date_time_excel'], inplace=True)\n",
        "    print(f\"[{time.time() - start_time:.1f}s] Excel date converted.\")\n",
        "    print(f\"[{time.time() - start_time:.1f}s] Loaded and initially processed: {climate_filename}\")\n",
        "\n",
        "\n",
        "    # --- Load Sample Submission (CSV) ---\n",
        "    sample_sub_filename = 'SampleSubmission.csv'\n",
        "    sample_sub_path = os.path.join(DATA_DIR, sample_sub_filename)\n",
        "    print(f\"\\n[{time.time() - start_time:.1f}s] Attempting to load sample submission from: {sample_sub_path}\")\n",
        "    sample_sub = pd.read_csv(sample_sub_path)\n",
        "    print(f\"[{time.time() - start_time:.1f}s] Successfully loaded: {sample_sub_filename}\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n[{total_time:.1f}s] All data loaded successfully.\")\n",
        "\n",
        "# --- Keep the SAME except blocks as before ---\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"\\n--- ERROR: File Not Found ---\")\n",
        "    print(f\"{e}\")\n",
        "    print(f\"Please ensure files are directly in the '{DATA_DIR}' directory confirmed in Step 1.\")\n",
        "    raise\n",
        "except ImportError as e:\n",
        "     print(f\"\\n--- ERROR: Missing Library ---\")\n",
        "     print(f\"{e}\")\n",
        "     print(\"Failed loading Excel file. Ensure openpyxl is installed (`!pip install openpyxl`).\")\n",
        "     raise\n",
        "except ValueError as e:\n",
        "    print(f\"\\n--- ERROR: Value Error during loading/parsing ---\")\n",
        "    print(f\"{e}\")\n",
        "    print(\"This might be due to an incorrect column name during processing. Verify names from Step 2.\")\n",
        "    raise\n",
        "except KeyError as e:\n",
        "    print(f\"\\n--- ERROR: Key Error ---\")\n",
        "    print(f\"{e}\")\n",
        "    print(\"This usually means a specified column name was not found during renaming or processing. Verify names from Step 2.\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "     print(f\"\\n--- ERROR: An unexpected error occurred during data loading ---\")\n",
        "     print(f\"{e}\")\n",
        "     raise\n",
        "\n",
        "# --- Display Info (Optional but recommended) ---\n",
        "# Select only the columns we will actually use going forward to keep info clean\n",
        "mhp_final_cols = ['timestamp', 'user_id', 'kwh']\n",
        "climate_final_cols = ['timestamp', 'temperature', 'dew_point', 'wind_speed', 'precipitation'] # Add others like 'snowfall' if needed\n",
        "\n",
        "if 'mhp_data_raw' in locals():\n",
        "    print(\"\\n--- MHP Data Info (Post-Processing) ---\")\n",
        "    mhp_data_raw[mhp_final_cols].info() # Show info only for relevant columns\n",
        "if 'climate_data_raw' in locals():\n",
        "    print(\"\\n--- Climate Data Info (Post-Processing) ---\")\n",
        "    climate_data_raw[climate_final_cols].info() # Show info only for relevant columns\n",
        "if 'sample_sub' in locals():\n",
        "    print(\"\\n--- Sample Submission Head ---\")\n",
        "    print(sample_sub.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2115812",
      "metadata": {
        "id": "f2115812"
      },
      "source": [
        "## 3. Data Preprocessing and Aggregation\n",
        "\n",
        "The MHP data is recorded at 5-minute intervals, but the prediction target is daily kWh per user. Climate data might also be at a finer granularity than daily.\n",
        "\n",
        "We need to:\n",
        "1.  Aggregate the MHP `kwh` readings to get the total daily sum for each `user_id`.\n",
        "2.  Aggregate the climate data to daily statistics (e.g., mean temperature, total precipitation).\n",
        "3.  Merge the aggregated daily MHP data with the aggregated daily climate data.\n",
        "4.  Handle any missing values that might arise from the merge or exist in the original data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c2981e61",
      "metadata": {
        "id": "c2981e61",
        "outputId": "821771e7-bde4-45f0-86f2-caccc44048fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aggregating data to daily level...\n",
            "\n",
            "[17.2s] Grouping MHP data by user_id and date...\n",
            "[31.6s] MHP aggregation complete.\n",
            "Aggregated MHP data shape: (136409, 3)\n",
            "Aggregated Daily kWh per User Head:\n",
            "                          user_id       date  daily_kwh\n",
            "0  consumer_device_10_data_user_1 2024-07-22   0.024330\n",
            "1  consumer_device_10_data_user_1 2024-07-23   0.103560\n",
            "2  consumer_device_10_data_user_1 2024-07-24   0.137543\n",
            "3  consumer_device_10_data_user_1 2024-07-25   0.121011\n",
            "4  consumer_device_10_data_user_1 2024-07-26   0.000000\n",
            "\n",
            "[31.6s] Aggregating climate data...\n",
            "[31.7s] Climate aggregation complete.\n",
            "Aggregated climate data shape: (511, 5)\n",
            "Columns after renaming: {daily_climate.columns.tolist()}\n",
            "Aggregated Daily Climate Head:\n",
            "        date  temp_mean  dew_point_mean  wind_speed_mean  precip_sum\n",
            "0 2023-06-03   1.860280       -3.348664         0.680343    0.004557\n",
            "1 2023-06-04   3.992740       -1.905203         0.672389    0.024096\n",
            "2 2023-06-05   4.794523       -3.781657         0.679359    0.011580\n",
            "3 2023-06-06   6.304390       -4.670615         0.574538    0.008914\n",
            "4 2023-06-07   7.003922       -3.965763         0.640492    0.008649\n",
            "\n",
            "[31.7s] Merging aggregated daily MHP and climate data...\n",
            "[31.7s] Merge complete. Initial merged shape: (136409, 7)\n",
            "\n",
            "[31.7s] Checking NaNs before fill:\n",
            "user_id            0\n",
            "date               0\n",
            "daily_kwh          0\n",
            "temp_mean          0\n",
            "dew_point_mean     0\n",
            "wind_speed_mean    0\n",
            "precip_sum         0\n",
            "dtype: int64\n",
            "\n",
            "[31.8s] FillNA complete. NaNs remaining: 0\n",
            "\n",
            "[31.8s] Cell 6 processing finished.\n",
            "\n",
            "--- Merged & Cleaned Training Data Head ---\n",
            "                          user_id       date  daily_kwh  temp_mean  \\\n",
            "0  consumer_device_10_data_user_1 2024-07-22   0.024330  14.719596   \n",
            "1  consumer_device_10_data_user_1 2024-07-23   0.103560  13.217268   \n",
            "2  consumer_device_10_data_user_1 2024-07-24   0.137543  12.462190   \n",
            "3  consumer_device_10_data_user_1 2024-07-25   0.121011  13.867551   \n",
            "4  consumer_device_10_data_user_1 2024-07-26   0.000000  15.572609   \n",
            "\n",
            "   dew_point_mean  wind_speed_mean  precip_sum  \n",
            "0        8.280669         0.439644    0.073049  \n",
            "1        9.862700         0.547125    0.121921  \n",
            "2        9.865658         0.656740    0.119984  \n",
            "3        8.973798         0.661945    0.034283  \n",
            "4        9.434734         0.788089    0.006961  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-1e8fee3ca695>:92: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df_train_full.fillna(method='ffill', inplace=True)\n",
            "<ipython-input-7-1e8fee3ca695>:93: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df_train_full.fillna(method='bfill', inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Data Preprocessing and Aggregation (Corrected Syntax)\n",
        "\n",
        "print(\"Aggregating data to daily level...\")\n",
        "agg_start_time = time.time() # Start timer for aggregation\n",
        "\n",
        "# --- MHP Data Aggregation ---\n",
        "# Ensure required columns exist from Cell 4 processing\n",
        "if 'timestamp' not in mhp_data_raw.columns: raise KeyError(\"Column 'timestamp' not found in mhp_data_raw.\")\n",
        "if 'user_id' not in mhp_data_raw.columns: raise KeyError(\"Column 'user_id' not found in mhp_data_raw.\")\n",
        "if 'kwh' not in mhp_data_raw.columns: raise KeyError(\"Column 'kwh' not found in mhp_data_raw.\")\n",
        "\n",
        "mhp_data = mhp_data_raw.copy()\n",
        "# Extract date from the timestamp\n",
        "mhp_data['date'] = mhp_data['timestamp'].dt.date\n",
        "\n",
        "# Group by the user ID (already named 'user_id') and the date, then sum the kWh for that day\n",
        "print(f\"\\n[{time.time() - agg_start_time:.1f}s] Grouping MHP data by user_id and date...\")\n",
        "# This can be memory intensive for 40M rows. Monitor Colab RAM.\n",
        "daily_kwh = mhp_data.groupby(['user_id', 'date'])['kwh'].sum().reset_index()\n",
        "print(f\"[{time.time() - agg_start_time:.1f}s] MHP aggregation complete.\")\n",
        "daily_kwh.rename(columns={'kwh': 'daily_kwh'}, inplace=True)\n",
        "# Convert date back to datetime object for merging and feature engineering\n",
        "daily_kwh['date'] = pd.to_datetime(daily_kwh['date'])\n",
        "print(f\"Aggregated MHP data shape: {daily_kwh.shape}\")\n",
        "print(\"Aggregated Daily kWh per User Head:\")\n",
        "print(daily_kwh.head())\n",
        "\n",
        "\n",
        "# --- Climate Data Aggregation (Using simplified names from Cell 4) ---\n",
        "# --- Climate Data Aggregation (Using simplified names from Cell 4 - CORRECTED .agg() call) ---\n",
        "# Ensure required columns exist\n",
        "if 'timestamp' not in climate_data_raw.columns: raise KeyError(\"Column 'timestamp' not found in climate_data_raw.\")\n",
        "\n",
        "climate_data = climate_data_raw.copy()\n",
        "# Extract date from the timestamp\n",
        "climate_data['date'] = climate_data['timestamp'].dt.date\n",
        "\n",
        "# Define the columns to aggregate and the aggregation function\n",
        "climate_agg_dict = {\n",
        "    'temperature': 'mean',\n",
        "    'dew_point': 'mean',\n",
        "    'wind_speed': 'mean',\n",
        "    'precipitation': 'sum'\n",
        "    # Add 'u_wind':'mean', 'v_wind':'mean' if you want to keep daily components\n",
        "}\n",
        "# Check if needed columns exist in climate_data before aggregation\n",
        "required_climate_cols = list(climate_agg_dict.keys())\n",
        "missing_climate_cols = [col for col in required_climate_cols if col not in climate_data.columns]\n",
        "if missing_climate_cols:\n",
        "    raise KeyError(f\"Missing required climate columns for aggregation: {missing_climate_cols}. Check Cell 4 processing.\")\n",
        "\n",
        "print(f\"\\n[{time.time() - agg_start_time:.1f}s] Aggregating climate data...\")\n",
        "\n",
        "# --- CORRECTED .agg() call ---\n",
        "# Pass the dictionary directly as the argument\n",
        "daily_climate = climate_data.groupby('date').agg(climate_agg_dict).reset_index()\n",
        "\n",
        "# Rename columns to include the aggregation type (e.g., temp_mean) - important!\n",
        "# Create the names expected by later cells (e.g., 'temp_mean', not 'temperature_mean')\n",
        "rename_map = {\n",
        "    'temperature': 'temp_mean',\n",
        "    'dew_point': 'dew_point_mean',\n",
        "    'wind_speed': 'wind_speed_mean',\n",
        "    'precipitation': 'precip_sum'\n",
        "}\n",
        "# Only rename columns that were actually aggregated\n",
        "rename_map = {k: v for k, v in rename_map.items() if k in daily_climate.columns}\n",
        "daily_climate.rename(columns=rename_map, inplace=True)\n",
        "# --- End of Correction ---\n",
        "\n",
        "\n",
        "print(f\"[{time.time() - agg_start_time:.1f}s] Climate aggregation complete.\")\n",
        "\n",
        "# Convert date back to datetime object\n",
        "daily_climate['date'] = pd.to_datetime(daily_climate['date'])\n",
        "print(f\"Aggregated climate data shape: {daily_climate.shape}\")\n",
        "print(\"Columns after renaming: {daily_climate.columns.tolist()}\") # Verify names\n",
        "print(\"Aggregated Daily Climate Head:\")\n",
        "print(daily_climate.head())\n",
        "\n",
        "\n",
        "\n",
        "# --- Merge Aggregated Data ---\n",
        "print(f\"\\n[{time.time() - agg_start_time:.1f}s] Merging aggregated daily MHP and climate data...\")\n",
        "df_train_full = pd.merge(daily_kwh, daily_climate, on='date', how='left')\n",
        "print(f\"[{time.time() - agg_start_time:.1f}s] Merge complete. Initial merged shape: {df_train_full.shape}\")\n",
        "\n",
        "# --- Handle Missing Values ---\n",
        "print(f\"\\n[{time.time() - agg_start_time:.1f}s] Checking NaNs before fill:\\n{df_train_full.isnull().sum()}\")\n",
        "# Sort before filling for consistent ffill/bfill\n",
        "df_train_full.sort_values(by=['user_id', 'date'], inplace=True)\n",
        "df_train_full.fillna(method='ffill', inplace=True)\n",
        "df_train_full.fillna(method='bfill', inplace=True)\n",
        "nan_count_after = df_train_full.isnull().sum().sum()\n",
        "print(f\"\\n[{time.time() - agg_start_time:.1f}s] FillNA complete. NaNs remaining: {nan_count_after}\")\n",
        "\n",
        "agg_total_time = time.time() - agg_start_time\n",
        "print(f\"\\n[{agg_total_time:.1f}s] Cell 6 processing finished.\")\n",
        "print(\"\\n--- Merged & Cleaned Training Data Head ---\")\n",
        "print(df_train_full.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "729be8bf",
      "metadata": {
        "id": "729be8bf"
      },
      "source": [
        "## 4. Feature Engineering\n",
        "\n",
        "Create features based on the date and potentially other aspects of the data. For this baseline, we will focus on date-based features. More advanced features (lags, rolling windows, user-specific stats) can be added later for improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "49f9eba0",
      "metadata": {
        "id": "49f9eba0",
        "outputId": "f4983018-0e64-4a6a-9643-2812426c568c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Step 0: Converting user_id to Category ---\n",
            "Original user_id dtype: category\n",
            "Converted user_id dtype: category\n",
            "\n",
            "--- Step 1: Creating Lag Features ---\n",
            "Sorting data by user_id and date...\n",
            "Calculating lags for column: 'daily_kwh' using periods: [1, 2, 3, 7, 14, 28]\n",
            "[0.1s] Created lag features: ['daily_kwh_lag_1', 'daily_kwh_lag_2', 'daily_kwh_lag_3', 'daily_kwh_lag_7', 'daily_kwh_lag_14', 'daily_kwh_lag_28']\n",
            "\n",
            "--- Step 3: Creating Date Features ---\n",
            "[0.1s] Date features applied.\n",
            "\n",
            "--- Step 4: Defining Final Feature List (NO Rolling Features) ---\n",
            "Added 'user_id' to features list.\n",
            "Adding 6 lag features.\n",
            "--- Rolling window features EXCLUDED for this experiment ---\n",
            "\n",
            "Final list of features DEFINED (19): ['year', 'month', 'day', 'dayofweek', 'dayofyear', 'weekofyear', 'quarter', 'is_weekend', 'temp_mean', 'dew_point_mean', 'wind_speed_mean', 'precip_sum', 'user_id', 'daily_kwh_lag_1', 'daily_kwh_lag_2', 'daily_kwh_lag_3', 'daily_kwh_lag_7', 'daily_kwh_lag_14', 'daily_kwh_lag_28']\n",
            "\n",
            "--- Step 5: Checking Data Types & Handling NaNs ---\n",
            "Checking for non-numeric feature columns (excluding category)...\n",
            "\n",
            "Handling remaining NaNs in features (filling with 0)...\n",
            "NaN counts BEFORE fill (numeric features only):\n",
            "daily_kwh_lag_1       585\n",
            "daily_kwh_lag_2      1170\n",
            "daily_kwh_lag_3      1755\n",
            "daily_kwh_lag_7      4095\n",
            "daily_kwh_lag_14     8190\n",
            "daily_kwh_lag_28    16380\n",
            "dtype: int64\n",
            "\n",
            "[ OK ] NaNs handled. Total remaining (numeric): 0\n",
            "\n",
            "[0.2s] Cell 8 processing finished.\n",
            "\n",
            "--- Training Data with Features Head (Sample) ---\n",
            "                          user_id       date  daily_kwh  year  month  \\\n",
            "0  consumer_device_10_data_user_1 2024-07-22   0.024330  2024      7   \n",
            "1  consumer_device_10_data_user_1 2024-07-23   0.103560  2024      7   \n",
            "2  consumer_device_10_data_user_1 2024-07-24   0.137543  2024      7   \n",
            "3  consumer_device_10_data_user_1 2024-07-25   0.121011  2024      7   \n",
            "4  consumer_device_10_data_user_1 2024-07-26   0.000000  2024      7   \n",
            "\n",
            "                          user_id  daily_kwh_lag_1  daily_kwh_lag_2  \n",
            "0  consumer_device_10_data_user_1         0.000000         0.000000  \n",
            "1  consumer_device_10_data_user_1         0.024330         0.000000  \n",
            "2  consumer_device_10_data_user_1         0.103560         0.024330  \n",
            "3  consumer_device_10_data_user_1         0.137543         0.103560  \n",
            "4  consumer_device_10_data_user_1         0.121011         0.137543  \n"
          ]
        }
      ],
      "source": [
        "# Cell 8: Feature Engineering (Date + Lag + UserID - NO Rolling)\n",
        "\n",
        "import time\n",
        "import pandas as pd # Ensure pandas is imported\n",
        "import numpy as np  # Ensure numpy is imported\n",
        "\n",
        "feature_eng_start_time = time.time() # Start timer\n",
        "\n",
        "# --- Check for input DataFrame ---\n",
        "if 'df_train_full' not in locals():\n",
        "    raise NameError(\"df_train_full not found. Ensure Cell 6 ran successfully.\")\n",
        "\n",
        "# --- 0. Convert user_id to Category dtype ---\n",
        "print(\"--- Step 0: Converting user_id to Category ---\")\n",
        "if 'user_id' in df_train_full.columns:\n",
        "    print(f\"Original user_id dtype: {df_train_full['user_id'].dtype}\")\n",
        "    df_train_full['user_id'] = df_train_full['user_id'].astype('category')\n",
        "    print(f\"Converted user_id dtype: {df_train_full['user_id'].dtype}\")\n",
        "else:\n",
        "    raise KeyError(\"Column 'user_id' not found in df_train_full.\")\n",
        "\n",
        "\n",
        "# --- 1. Create Lag Features ---\n",
        "print(\"\\n--- Step 1: Creating Lag Features ---\")\n",
        "print(\"Sorting data by user_id and date...\")\n",
        "df_train_full = df_train_full.sort_values(by=['user_id', 'date'])\n",
        "\n",
        "lags_to_create = [1, 2, 3, 7, 14, 28] # Lag periods in days\n",
        "lag_col_name_base = 'daily_kwh_lag_'\n",
        "lag_cols_created = [] # Keep track of new lag column names\n",
        "\n",
        "target_col_for_lags = 'daily_kwh'\n",
        "print(f\"Calculating lags for column: '{target_col_for_lags}' using periods: {lags_to_create}\")\n",
        "for lag in lags_to_create:\n",
        "    col_name = f\"{lag_col_name_base}{lag}\"\n",
        "    # print(f\"  Creating lag: {lag} days -> {col_name}\") # Reduce verbosity\n",
        "    # Use observed=True for category dtype groupbys\n",
        "    df_train_full[col_name] = df_train_full.groupby('user_id', observed=True)[target_col_for_lags].shift(lag)\n",
        "    lag_cols_created.append(col_name)\n",
        "print(f\"[{time.time() - feature_eng_start_time:.1f}s] Created lag features: {lag_cols_created}\")\n",
        "\n",
        "\n",
        "# --- 2. Create Rolling Window Features (COMMENTED OUT FOR NOW) ---\n",
        "# print(\"\\n--- Step 2: Creating Rolling Window Features (SKIPPED) ---\")\n",
        "# rolling_cols_created = [] # Ensure list exists even if empty\n",
        "# (Keep the rolling feature code commented out or deleted)\n",
        "\n",
        "\n",
        "# --- 3. Create Date Features ---\n",
        "print(\"\\n--- Step 3: Creating Date Features ---\")\n",
        "def create_date_features(df, date_col='date'):\n",
        "    \"\"\"Creates time series features from a date column.\"\"\"\n",
        "    df = df.copy()\n",
        "    df[date_col] = pd.to_datetime(df[date_col])\n",
        "    df['year'] = df[date_col].dt.year\n",
        "    df['month'] = df[date_col].dt.month\n",
        "    df['day'] = df[date_col].dt.day\n",
        "    df['dayofweek'] = df[date_col].dt.dayofweek\n",
        "    df['dayofyear'] = df[date_col].dt.dayofyear\n",
        "    df['weekofyear'] = df[date_col].dt.isocalendar().week.astype(int)\n",
        "    df['quarter'] = df[date_col].dt.quarter\n",
        "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
        "    return df\n",
        "\n",
        "df_train_full = create_date_features(df_train_full, 'date')\n",
        "print(f\"[{time.time() - feature_eng_start_time:.1f}s] Date features applied.\")\n",
        "\n",
        "\n",
        "# --- 4. Define Final Feature List ---\n",
        "print(\"\\n--- Step 4: Defining Final Feature List (NO Rolling Features) ---\")\n",
        "base_features = [\n",
        "    'year', 'month', 'day', 'dayofweek', 'dayofyear', 'weekofyear', 'quarter', 'is_weekend',\n",
        "    'temp_mean', 'dew_point_mean', 'wind_speed_mean', 'precip_sum'\n",
        "]\n",
        "features = list(base_features) # Start with base\n",
        "\n",
        "# Add user_id feature\n",
        "features.append('user_id')\n",
        "print(\"Added 'user_id' to features list.\")\n",
        "\n",
        "# Add lag features\n",
        "if 'lag_cols_created' in locals() and isinstance(lag_cols_created, list):\n",
        "    print(f\"Adding {len(lag_cols_created)} lag features.\")\n",
        "    features.extend(lag_cols_created)\n",
        "else: print(\"WARNING: Lag features ('lag_cols_created') not found or not a list.\")\n",
        "\n",
        "# --- ROLLING FEATURES EXCLUDED ---\n",
        "print(\"--- Rolling window features EXCLUDED for this experiment ---\")\n",
        "# if 'rolling_cols_created' in locals() and isinstance(rolling_cols_created, list):\n",
        "#    features.extend(rolling_cols_created) # Ensure this is commented out/deleted\n",
        "\n",
        "target = 'daily_kwh'\n",
        "print(f\"\\nFinal list of features DEFINED ({len(features)}): {features}\") # Check this output\n",
        "\n",
        "\n",
        "# --- 5. Check Feature Data Types and Handle NaNs ---\n",
        "print(\"\\n--- Step 5: Checking Data Types & Handling NaNs ---\")\n",
        "missing_features = [f for f in features if f not in df_train_full.columns]\n",
        "if missing_features: raise ValueError(f\"Features defined but missing: {missing_features}\")\n",
        "\n",
        "print(\"Checking for non-numeric feature columns (excluding category)...\")\n",
        "for col in features:\n",
        "    if col != 'user_id' and not pd.api.types.is_numeric_dtype(df_train_full[col]):\n",
        "        print(f\"  Warning: Feature '{col}' not numeric ({df_train_full[col].dtype}). Converting.\")\n",
        "        df_train_full[col] = pd.to_numeric(df_train_full[col], errors='coerce')\n",
        "        if df_train_full[col].isnull().any():\n",
        "             print(f\"    NaNs from conversion in {col}, filling with 0.\")\n",
        "             df_train_full[col].fillna(0, inplace=True)\n",
        "\n",
        "print(f\"\\nHandling remaining NaNs in features (filling with 0)...\")\n",
        "numeric_features_for_nan_fill = [f for f in features if f != 'user_id']\n",
        "nan_counts_before_fill = df_train_full[numeric_features_for_nan_fill].isnull().sum()\n",
        "print(\"NaN counts BEFORE fill (numeric features only):\")\n",
        "print(nan_counts_before_fill[nan_counts_before_fill > 0])\n",
        "df_train_full[numeric_features_for_nan_fill] = df_train_full[numeric_features_for_nan_fill].fillna(0)\n",
        "nan_counts_after_fill = df_train_full[numeric_features_for_nan_fill].isnull().sum().sum()\n",
        "if nan_counts_after_fill == 0: print(f\"\\n[ OK ] NaNs handled. Total remaining (numeric): {nan_counts_after_fill}\")\n",
        "else: print(f\"\\n[ WARNING ] NaNs remain after fill (numeric): {nan_counts_after_fill}.\")\n",
        "\n",
        "feature_eng_total_time = time.time() - feature_eng_start_time\n",
        "print(f\"\\n[{feature_eng_total_time:.1f}s] Cell 8 processing finished.\")\n",
        "\n",
        "print(\"\\n--- Training Data with Features Head (Sample) ---\")\n",
        "# Display head including user_id and some lags\n",
        "cols_to_show = (['user_id', 'date', target] + base_features[:2] + ['user_id'] +\n",
        "                lag_cols_created[:2] ) # Removed rolling cols from print\n",
        "print(df_train_full[cols_to_show].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a487309a",
      "metadata": {
        "id": "a487309a"
      },
      "source": [
        "## 5. Train/Validation Split (Time-Based)\n",
        "\n",
        "For time series forecasting, it's crucial to validate the model on data that comes *after* the training data. We will split the data chronologically, using the most recent period for validation. Shuffling should **not** be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "fd68efd1",
      "metadata": {
        "id": "fd68efd1",
        "outputId": "1da531aa-aa31-4426-84f8-33fadd836266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833,
          "referenced_widgets": [
            "60c2519fc1d049ee821397dd19805236",
            "78ec9de80b194802b77e9de64d56973f",
            "82fd74673b3a4a8080e00a95759bb1a6",
            "009f97a1d6564b18ad848adb72c7b3bd",
            "122a1282f64e431294c4cacdd73d33cd",
            "188d84ce1a04404c82f9df1c2f9dec45",
            "e3066e11c14d40f387293471394c1c4a",
            "b6a59c07a4d44ca5b12a25ae256fb7ed",
            "876d51e2a5d340c0a8d7421f334a0ee1",
            "c8abe1bc4c8f4bc3baf69943021cfbcb",
            "96f8a848863942c6a3e3fa1b63534976"
          ]
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-06 13:32:04,847] A new study created in memory with name: no-name-b9c32933-f481-43d1-a0fd-aad80876c0e2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Optuna Hyperparameter Tuning ---\n",
            "Data shape for Optuna: X=(136409, 19), y=(136409,)\n",
            "Features used (19): ['year', 'month', 'day', 'dayofweek', 'dayofyear', 'weekofyear', 'quarter', 'is_weekend', 'temp_mean', 'dew_point_mean', 'wind_speed_mean', 'precip_sum', 'user_id', 'daily_kwh_lag_1', 'daily_kwh_lag_2', 'daily_kwh_lag_3', 'daily_kwh_lag_7', 'daily_kwh_lag_14', 'daily_kwh_lag_28']\n",
            "user_id dtype in X_full_opt: category\n",
            "\n",
            "Running Optuna study with 20 trials...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60c2519fc1d049ee821397dd19805236"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[I 2025-04-06 13:32:46,699] Trial 0 finished with value: 3.1893540520491173 and parameters: {'learning_rate': 0.029360913980860538, 'num_leaves': 24, 'max_depth': 10, 'feature_fraction': 0.7282883809023775, 'bagging_fraction': 0.542364078616247, 'bagging_freq': 2, 'lambda_l1': 9.75148305736885, 'lambda_l2': 0.19982594752523308, 'min_child_samples': 29}. Best is trial 0 with value: 3.1893540520491173.\n",
            "[I 2025-04-06 13:33:17,955] Trial 1 finished with value: 2.85457616805172 and parameters: {'learning_rate': 0.043793439626086006, 'num_leaves': 33, 'max_depth': 4, 'feature_fraction': 0.5715880746686312, 'bagging_fraction': 0.7992913866343199, 'bagging_freq': 2, 'lambda_l1': 0.35963564290540034, 'lambda_l2': 0.39329506131320896, 'min_child_samples': 21}. Best is trial 1 with value: 2.85457616805172.\n",
            "[I 2025-04-06 13:33:56,790] Trial 2 finished with value: 2.852914189692552 and parameters: {'learning_rate': 0.08350534311758406, 'num_leaves': 23, 'max_depth': 10, 'feature_fraction': 0.7943356732418081, 'bagging_fraction': 0.7082610497648534, 'bagging_freq': 7, 'lambda_l1': 4.671635027873067, 'lambda_l2': 0.36700526153877294, 'min_child_samples': 20}. Best is trial 2 with value: 2.852914189692552.\n",
            "[I 2025-04-06 13:34:17,552] Trial 3 finished with value: 2.7949052178521385 and parameters: {'learning_rate': 0.08549815748136078, 'num_leaves': 54, 'max_depth': 3, 'feature_fraction': 0.5957319216854958, 'bagging_fraction': 0.7013623222636498, 'bagging_freq': 6, 'lambda_l1': 1.2781273543079634, 'lambda_l2': 0.7806849364957191, 'min_child_samples': 48}. Best is trial 3 with value: 2.7949052178521385.\n",
            "[I 2025-04-06 13:34:58,442] Trial 4 finished with value: 3.0864976913421995 and parameters: {'learning_rate': 0.027548257511211913, 'num_leaves': 14, 'max_depth': 5, 'feature_fraction': 0.8910078118444449, 'bagging_fraction': 0.6215742706263738, 'bagging_freq': 5, 'lambda_l1': 0.19201327462238738, 'lambda_l2': 0.8190821195064997, 'min_child_samples': 30}. Best is trial 3 with value: 2.7949052178521385.\n",
            "[I 2025-04-06 13:35:48,698] Trial 5 finished with value: 2.845176214900219 and parameters: {'learning_rate': 0.013367843530505826, 'num_leaves': 28, 'max_depth': 3, 'feature_fraction': 0.5559660187524207, 'bagging_fraction': 0.7994077091544785, 'bagging_freq': 2, 'lambda_l1': 0.35824568201160595, 'lambda_l2': 0.5669200837056446, 'min_child_samples': 7}. Best is trial 3 with value: 2.7949052178521385.\n",
            "[I 2025-04-06 13:36:22,942] Trial 6 finished with value: 2.857920792113486 and parameters: {'learning_rate': 0.055913930007459166, 'num_leaves': 51, 'max_depth': 5, 'feature_fraction': 0.5256262700031088, 'bagging_fraction': 0.5086898102338928, 'bagging_freq': 3, 'lambda_l1': 4.569507237063888, 'lambda_l2': 4.024318201506993, 'min_child_samples': 48}. Best is trial 3 with value: 2.7949052178521385.\n",
            "[I 2025-04-06 13:36:48,319] Trial 7 finished with value: 2.243520795258683 and parameters: {'learning_rate': 0.09949767459513811, 'num_leaves': 37, 'max_depth': 5, 'feature_fraction': 0.6965403421234245, 'bagging_fraction': 0.8424168133233703, 'bagging_freq': 4, 'lambda_l1': 0.7161864500117657, 'lambda_l2': 2.5144402982974667, 'min_child_samples': 36}. Best is trial 7 with value: 2.243520795258683.\n",
            "[I 2025-04-06 13:37:31,839] Trial 8 finished with value: 3.1630262189731844 and parameters: {'learning_rate': 0.011168195638955608, 'num_leaves': 15, 'max_depth': 4, 'feature_fraction': 0.8505002099200321, 'bagging_fraction': 0.8190306224041557, 'bagging_freq': 1, 'lambda_l1': 3.6560496639314946, 'lambda_l2': 3.835760594969312, 'min_child_samples': 13}. Best is trial 7 with value: 2.243520795258683.\n",
            "[I 2025-04-06 13:38:01,999] Trial 9 finished with value: 2.9786965079264096 and parameters: {'learning_rate': 0.04038574588980056, 'num_leaves': 49, 'max_depth': 3, 'feature_fraction': 0.7563518071021194, 'bagging_fraction': 0.8948866903030439, 'bagging_freq': 4, 'lambda_l1': 0.11488221469179245, 'lambda_l2': 0.25927224718886016, 'min_child_samples': 7}. Best is trial 7 with value: 2.243520795258683.\n",
            "[I 2025-04-06 13:40:05,715] Trial 10 finished with value: 2.264575287782322 and parameters: {'learning_rate': 0.017963774031018336, 'num_leaves': 43, 'max_depth': 8, 'feature_fraction': 0.6747114509302778, 'bagging_fraction': 0.8971512216348908, 'bagging_freq': 4, 'lambda_l1': 1.097646725588729, 'lambda_l2': 9.253471975853612, 'min_child_samples': 38}. Best is trial 7 with value: 2.243520795258683.\n",
            "[I 2025-04-06 13:41:42,710] Trial 11 finished with value: 2.277040881798679 and parameters: {'learning_rate': 0.018637280493990466, 'num_leaves': 40, 'max_depth': 8, 'feature_fraction': 0.6539765646297968, 'bagging_fraction': 0.8959684454622441, 'bagging_freq': 4, 'lambda_l1': 1.0718804144045995, 'lambda_l2': 9.815387244516169, 'min_child_samples': 37}. Best is trial 7 with value: 2.243520795258683.\n",
            "[I 2025-04-06 13:42:42,183] Trial 12 finished with value: 2.148062881281148 and parameters: {'learning_rate': 0.018751447601805003, 'num_leaves': 40, 'max_depth': 7, 'feature_fraction': 0.6631647641150664, 'bagging_fraction': 0.8388570087384377, 'bagging_freq': 5, 'lambda_l1': 0.6556539409810125, 'lambda_l2': 2.1267338869864436, 'min_child_samples': 39}. Best is trial 12 with value: 2.148062881281148.\n",
            "[I 2025-04-06 13:44:09,605] Trial 13 finished with value: 2.1641509533553314 and parameters: {'learning_rate': 0.02431840990288337, 'num_leaves': 39, 'max_depth': 7, 'feature_fraction': 0.6378565831743453, 'bagging_fraction': 0.7549973541260837, 'bagging_freq': 6, 'lambda_l1': 0.5253918675412157, 'lambda_l2': 1.8696065755752165, 'min_child_samples': 39}. Best is trial 12 with value: 2.148062881281148.\n",
            "[I 2025-04-06 13:45:31,557] Trial 14 finished with value: 2.2622292300885265 and parameters: {'learning_rate': 0.021383851964528633, 'num_leaves': 59, 'max_depth': 7, 'feature_fraction': 0.6195046085909285, 'bagging_fraction': 0.7415476141451669, 'bagging_freq': 6, 'lambda_l1': 0.5089987848099637, 'lambda_l2': 1.7450385420439285, 'min_child_samples': 41}. Best is trial 12 with value: 2.148062881281148.\n",
            "[I 2025-04-06 13:46:46,605] Trial 15 finished with value: 2.23259104162315 and parameters: {'learning_rate': 0.02256328021829117, 'num_leaves': 45, 'max_depth': 7, 'feature_fraction': 0.637346858247301, 'bagging_fraction': 0.7545848300622249, 'bagging_freq': 7, 'lambda_l1': 2.1603185133991163, 'lambda_l2': 0.1034113073807826, 'min_child_samples': 43}. Best is trial 12 with value: 2.148062881281148.\n",
            "[I 2025-04-06 13:48:29,315] Trial 16 finished with value: 3.044331373170008 and parameters: {'learning_rate': 0.01367522517517787, 'num_leaves': 32, 'max_depth': 8, 'feature_fraction': 0.7620241292891452, 'bagging_fraction': 0.6460418700116386, 'bagging_freq': 6, 'lambda_l1': 0.2118613165426532, 'lambda_l2': 1.40388996857445, 'min_child_samples': 31}. Best is trial 12 with value: 2.148062881281148.\n",
            "[I 2025-04-06 13:49:22,949] Trial 17 finished with value: 3.017648855549286 and parameters: {'learning_rate': 0.039083678586042545, 'num_leaves': 38, 'max_depth': 6, 'feature_fraction': 0.5015896913414952, 'bagging_fraction': 0.7530318143453191, 'bagging_freq': 5, 'lambda_l1': 1.922030117467188, 'lambda_l2': 4.18556128522764, 'min_child_samples': 23}. Best is trial 12 with value: 2.148062881281148.\n",
            "[I 2025-04-06 13:51:47,313] Trial 18 finished with value: 2.9049770013526435 and parameters: {'learning_rate': 0.016348226196571874, 'num_leaves': 45, 'max_depth': 9, 'feature_fraction': 0.6998769301303805, 'bagging_fraction': 0.6309049627437575, 'bagging_freq': 5, 'lambda_l1': 0.6384812917457061, 'lambda_l2': 1.9726600209262455, 'min_child_samples': 43}. Best is trial 12 with value: 2.148062881281148.\n",
            "[I 2025-04-06 13:52:39,790] Trial 19 finished with value: 2.1313616058499494 and parameters: {'learning_rate': 0.023638976056168663, 'num_leaves': 31, 'max_depth': 6, 'feature_fraction': 0.606413813222159, 'bagging_fraction': 0.8463930721986744, 'bagging_freq': 6, 'lambda_l1': 0.31294092701733434, 'lambda_l2': 1.1947964494084333, 'min_child_samples': 34}. Best is trial 19 with value: 2.1313616058499494.\n",
            "\n",
            "--- Optuna Tuning Summary ---\n",
            "Number of finished trials: 20\n",
            "Best trial:\n",
            "  Value (Mean Realistic CV RMSE): 2.1314\n",
            "  Params: \n",
            "    learning_rate: 0.023638976056168663\n",
            "    num_leaves: 31\n",
            "    max_depth: 6\n",
            "    feature_fraction: 0.606413813222159\n",
            "    bagging_fraction: 0.8463930721986744\n",
            "    bagging_freq: 6\n",
            "    lambda_l1: 0.31294092701733434\n",
            "    lambda_l2: 1.1947964494084333\n",
            "    min_child_samples: 34\n",
            "\n",
            "Optuna tuning finished in 1235.0s\n"
          ]
        }
      ],
      "source": [
        "# Cell 9A: Optuna Hyperparameter Tuning (Corrected for user_id handling)\n",
        "\n",
        "import optuna\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd # Ensure pandas is imported\n",
        "import time\n",
        "\n",
        "print(\"--- Starting Optuna Hyperparameter Tuning ---\")\n",
        "optuna_start_time = time.time()\n",
        "\n",
        "# --- Check for required inputs from Cell 8 ---\n",
        "if 'df_train_full' not in locals(): raise NameError(\"df_train_full not found.\")\n",
        "# *** Crucially, use the 'features' list DEFINED AND FINALIZED in Cell 8 ***\n",
        "if 'features' not in locals(): raise NameError(\"features list not defined.\")\n",
        "if 'target' not in locals(): raise NameError(\"target variable name not defined.\")\n",
        "if 'user_id' not in features: raise ValueError(\"'user_id' MUST be included in the 'features' list defined in Cell 8.\")\n",
        "if 'SEED' not in locals(): SEED = 42; print(\"Warning: SEED not found, using 42.\")\n",
        "\n",
        "# --- Prepare full dataset for Optuna ---\n",
        "# Ensure correct sorting and data types FROM Cell 8 output\n",
        "df_train_full_opt = df_train_full.sort_values(by='date') # Use the dataframe processed by Cell 8\n",
        "\n",
        "# Ensure user_id is category type *before* splitting\n",
        "if df_train_full_opt['user_id'].dtype.name != 'category':\n",
        "    print(\"Error: user_id dtype is not 'category' after Cell 8. Check Cell 8.\")\n",
        "    # Force conversion again just in case, but Cell 8 should handle it\n",
        "    df_train_full_opt['user_id'] = df_train_full_opt['user_id'].astype('category')\n",
        "\n",
        "# Create X and y using the DEFINITIVE features list from Cell 8\n",
        "X_full_opt = df_train_full_opt[features]\n",
        "y_full_opt = df_train_full_opt[target]\n",
        "print(f\"Data shape for Optuna: X={X_full_opt.shape}, y={y_full_opt.shape}\")\n",
        "print(f\"Features used ({len(features)}): {X_full_opt.columns.tolist()}\") # Verify features used\n",
        "# Verify user_id dtype again\n",
        "print(f\"user_id dtype in X_full_opt: {X_full_opt['user_id'].dtype}\") # Should be 'category'\n",
        "\n",
        "\n",
        "# --- Define Optuna Objective Function ---\n",
        "def objective(trial):\n",
        "    # --- Suggest Hyperparameters ---\n",
        "    params = {\n",
        "        'objective': 'regression_l1',\n",
        "        'metric': 'rmse',\n",
        "        'n_estimators': 2000,\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 10, 60),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 0.9),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 0.9),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
        "        'lambda_l1': trial.suggest_float('lambda_l1', 0.1, 10.0, log=True),\n",
        "        'lambda_l2': trial.suggest_float('lambda_l2', 0.1, 10.0, log=True),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
        "        'seed': SEED,\n",
        "        'n_jobs': -1,\n",
        "        'verbose': -1,\n",
        "        'boosting_type': 'gbdt'\n",
        "    }\n",
        "\n",
        "    # --- Run CV with suggested params ---\n",
        "    N_SPLITS_OPT = 5\n",
        "    tscv_opt = TimeSeriesSplit(n_splits=N_SPLITS_OPT)\n",
        "    fold_scores_opt = []\n",
        "    zero_rmse_folds_opt = []\n",
        "\n",
        "    # Use the prepared X_full_opt and y_full_opt which have correct dtypes\n",
        "    for fold, (train_index, val_index) in enumerate(tscv_opt.split(X_full_opt)):\n",
        "        X_train, X_val = X_full_opt.iloc[train_index], X_full_opt.iloc[val_index]\n",
        "        y_train, y_val = y_full_opt.iloc[train_index], y_full_opt.iloc[val_index]\n",
        "\n",
        "        # Initialize model for the fold\n",
        "        model_fold = lgb.LGBMRegressor(**params)\n",
        "\n",
        "        # Fit the model - NO 'categorical_feature' argument needed here\n",
        "        # because X_train['user_id'] already has 'category' dtype\n",
        "        try:\n",
        "            model_fold.fit(X_train, y_train,\n",
        "                           eval_set=[(X_val, y_val)],\n",
        "                           eval_metric='rmse',\n",
        "                           callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)]\n",
        "                           )\n",
        "        except Exception as e:\n",
        "            print(f\"!!! Error during model.fit in Trial {trial.number}, Fold {fold+1}: {e}\")\n",
        "            # Optionally return a high error value for this trial\n",
        "            return float('inf')\n",
        "\n",
        "\n",
        "        val_preds_fold = model_fold.predict(X_val)\n",
        "        val_preds_fold = np.maximum(0, val_preds_fold) # Clip negatives\n",
        "        # Handle potential NaNs in predictions if fit failed partially (unlikely with exception handling)\n",
        "        val_preds_fold = np.nan_to_num(val_preds_fold, nan=0.0) # Replace NaN preds with 0\n",
        "\n",
        "        # Ensure y_val has no NaNs for metric calculation\n",
        "        if y_val.isnull().any():\n",
        "             print(f\"Warning: NaNs found in y_val for Trial {trial.number}, Fold {fold+1}. Check data.\")\n",
        "             # Decide how to handle: skip fold, fill y_val (not recommended), or error\n",
        "             continue # Skip this fold if target is NaN\n",
        "\n",
        "        rmse_fold = math.sqrt(mean_squared_error(y_val, val_preds_fold))\n",
        "\n",
        "        if np.isclose(rmse_fold, 0.0) and y_val.nunique() > 1: # Check if truly zero or just constant target\n",
        "            print(f\"Warning: Fold {fold+1} in Trial {trial.number} has RMSE ~0 but target is not constant.\")\n",
        "            zero_rmse_folds_opt.append(fold)\n",
        "        elif y_val.nunique() <= 1 and np.isclose(rmse_fold, 0.0) :\n",
        "             # This is the expected zero fold case\n",
        "             zero_rmse_folds_opt.append(fold)\n",
        "\n",
        "        fold_scores_opt.append(rmse_fold)\n",
        "\n",
        "    # --- Calculate and Return Realistic CV Score ---\n",
        "    realistic_scores = [score for i, score in enumerate(fold_scores_opt) if i not in zero_rmse_folds_opt]\n",
        "    if not realistic_scores:\n",
        "        print(f\"Warning: Trial {trial.number} resulted in all zero/invalid RMSE folds.\")\n",
        "        # Check if fold_scores_opt has any non-zero values at all\n",
        "        if any(not np.isclose(s, 0.0) for s in fold_scores_opt):\n",
        "             # If there were non-zero scores but they were excluded, maybe use all scores\n",
        "             print(\"Using all fold scores as fallback.\")\n",
        "             mean_realistic_cv_rmse = np.mean(fold_scores_opt)\n",
        "        else:\n",
        "             # All scores were genuinely zero\n",
        "             mean_realistic_cv_rmse = 0.0 # Or return float('inf')? Let's return 0 for now.\n",
        "    else:\n",
        "        mean_realistic_cv_rmse = np.mean(realistic_scores)\n",
        "\n",
        "    # print(f\"Trial {trial.number} Realistic RMSE: {mean_realistic_cv_rmse:.4f}\") # Optional: progress print\n",
        "    return mean_realistic_cv_rmse\n",
        "\n",
        "# --- Create and Run Optuna Study ---\n",
        "N_TRIALS = 20 # Keep at 50 for now\n",
        "print(f\"\\nRunning Optuna study with {N_TRIALS} trials...\")\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
        "\n",
        "# --- Report Best Results ---\n",
        "print(\"\\n--- Optuna Tuning Summary ---\")\n",
        "print(f\"Number of finished trials: {len(study.trials)}\")\n",
        "if study.best_trial: # Check if a best trial exists\n",
        "    print(\"Best trial:\")\n",
        "    best_trial = study.best_trial\n",
        "    print(f\"  Value (Mean Realistic CV RMSE): {best_trial.value:.4f}\")\n",
        "    print(\"  Params: \")\n",
        "    for key, value in best_trial.params.items():\n",
        "        print(f\"    {key}: {value}\")\n",
        "    # Store best params for next step\n",
        "    best_params_from_optuna = best_trial.params\n",
        "else:\n",
        "    print(\"Study finished without finding a best trial (all might have failed).\")\n",
        "    best_params_from_optuna = None # Indicate failure\n",
        "\n",
        "optuna_time = time.time() - optuna_start_time\n",
        "print(f\"\\nOptuna tuning finished in {optuna_time:.1f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Temporary Cell (Run this INSTEAD of re-running Cell 9)\n",
        "\n",
        "print(\"--- Preparing X_full and y_full for Final Model Training ---\")\n",
        "\n",
        "# --- Check for required inputs ---\n",
        "if 'df_train_full' not in locals(): raise NameError(\"df_train_full not found (Run Cell 8).\")\n",
        "if 'features' not in locals(): raise NameError(\"features list not defined (Run Cell 8).\")\n",
        "if 'target' not in locals(): raise NameError(\"target variable name not defined (Run Cell 8).\")\n",
        "\n",
        "# --- Prepare dataset (Copied from start of Cell 9) ---\n",
        "# Ensure data is sorted by date\n",
        "print(\"Sorting data by date...\")\n",
        "df_train_full_final = df_train_full.sort_values(by='date') # Use a slightly different name\n",
        "\n",
        "# Ensure user_id is category type\n",
        "if df_train_full_final['user_id'].dtype.name != 'category':\n",
        "    print(\"Converting user_id to category...\")\n",
        "    df_train_full_final['user_id'] = df_train_full_final['user_id'].astype('category')\n",
        "\n",
        "# Define X_full and y_full using the feature list from Cell 8\n",
        "X_full = df_train_full_final[features] # Defines X_full\n",
        "y_full = df_train_full_final[target]   # Defines y_full\n",
        "\n",
        "print(f\"X_full defined. Shape: {X_full.shape}\")\n",
        "print(f\"y_full defined. Shape: {y_full.shape}\")\n",
        "print(f\"user_id dtype in X_full: {X_full['user_id'].dtype}\")\n",
        "print(\"--- Ready to run Cell 10 ---\")"
      ],
      "metadata": {
        "id": "mvFe7TeXQ8sW",
        "outputId": "8db31c33-f86b-4822-84f3-d64bcb3d96c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mvFe7TeXQ8sW",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Preparing X_full and y_full for Final Model Training ---\n",
            "Sorting data by date...\n",
            "X_full defined. Shape: (136409, 19)\n",
            "y_full defined. Shape: (136409,)\n",
            "user_id dtype in X_full: category\n",
            "--- Ready to run Cell 10 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c0e2e18",
      "metadata": {
        "id": "0c0e2e18"
      },
      "source": [
        "## 6. Model Training (optuna Baseline)\n",
        "\n",
        "We will use LightGBM, a gradient boosting framework known for its speed and efficiency, to train a baseline model. We'll use early stopping based on the validation set performance (RMSE) to prevent overfitting and find a reasonable number of boosting rounds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ec1e217c",
      "metadata": {
        "id": "ec1e217c",
        "outputId": "ecb9a8d2-13c1-4177-a9c0-d2c7d7ca5995",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training Final Model on Full Data using Optuna Params ---\n",
            "Using Optuna best params + n_estimators=800\n",
            "Full final parameters: {'learning_rate': 0.023638976056168663, 'num_leaves': 31, 'max_depth': 6, 'feature_fraction': 0.606413813222159, 'bagging_fraction': 0.8463930721986744, 'bagging_freq': 6, 'lambda_l1': 0.31294092701733434, 'lambda_l2': 1.1947964494084333, 'min_child_samples': 34, 'objective': 'regression_l1', 'metric': 'rmse', 'seed': 42, 'n_jobs': -1, 'verbose': -1, 'boosting_type': 'gbdt', 'n_estimators': 800}\n",
            "Training final model on 136409 samples...\n",
            "\n",
            "Final model training complete in 25.1s\n"
          ]
        }
      ],
      "source": [
        "# Cell 10: Final Model Training (Using Optuna Best Params)\n",
        "\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "print(\"\\n--- Training Final Model on Full Data using Optuna Params ---\")\n",
        "final_model_start_time = time.time()\n",
        "\n",
        "# --- Check for required inputs ---\n",
        "if 'X_full' not in locals(): raise NameError(\"X_full (features) not found.\")\n",
        "if 'y_full' not in locals(): raise NameError(\"y_full (target) not found.\")\n",
        "if 'best_params_from_optuna' not in locals(): raise NameError(\"best_params_from_optuna not found.\")\n",
        "# We still need best_iterations from the *initial* CV run (Cell 9) if we want to use median rounds\n",
        "# If Cell 9 wasn't re-run after Optuna, best_iterations might be from the old params.\n",
        "# Safer to train Optuna best params for a fixed large number or estimate rounds differently.\n",
        "# Let's train for a fixed, reasonably large number of rounds found during tuning.\n",
        "# Alternatively, refactor to get best rounds from the best Optuna trial if needed.\n",
        "\n",
        "# --- Initialize Final Model using Optuna Params ---\n",
        "final_model_params = best_params_from_optuna.copy()\n",
        "# Add back necessary fixed params if not tuned by Optuna\n",
        "final_model_params['objective'] = 'regression_l1'\n",
        "final_model_params['metric'] = 'rmse' # Can be omitted if not monitoring fit\n",
        "final_model_params['seed'] = SEED\n",
        "final_model_params['n_jobs'] = -1\n",
        "final_model_params['verbose'] = -1\n",
        "final_model_params['boosting_type'] = 'gbdt'\n",
        "# Set n_estimators - using a fixed high value or estimate from Optuna trials average\n",
        "# Let's use a fixed value based on previous reasonable results\n",
        "final_model_params['n_estimators'] = 800 # Adjust based on typical best_iterations seen before\n",
        "print(f\"Using Optuna best params + n_estimators={final_model_params['n_estimators']}\")\n",
        "print(f\"Full final parameters: {final_model_params}\")\n",
        "\n",
        "# Ensure user_id is category\n",
        "if X_full['user_id'].dtype.name != 'category':\n",
        "     print(\"Warning: Converting X_full user_id to category for final training.\")\n",
        "     X_full['user_id'] = X_full['user_id'].astype('category')\n",
        "\n",
        "final_model = lgb.LGBMRegressor(**final_model_params)\n",
        "\n",
        "# --- Train on ALL data ---\n",
        "print(f\"Training final model on {X_full.shape[0]} samples...\")\n",
        "final_model.fit(X_full, y_full,\n",
        "                categorical_feature=['user_id'] # Specify categorical feature\n",
        "               )\n",
        "\n",
        "final_model_time = time.time() - final_model_start_time\n",
        "print(f\"\\nFinal model training complete in {final_model_time:.1f}s\")\n",
        "\n",
        "# Optional: Save the final model\n",
        "# import joblib\n",
        "# joblib.dump(final_model, 'final_lgb_model_v7_optuna.pkl')\n",
        "# print(\"Final model saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bfaa893",
      "metadata": {
        "id": "8bfaa893"
      },
      "source": [
        "## 7. Local Validation (Calculate RMSE)\n",
        "\n",
        "Evaluate the trained model's performance on the unseen validation set. This gives us an estimate of how well the model might perform on the actual test data on Zindi. We will calculate the Root Mean Squared Error (RMSE)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a98dead",
      "metadata": {
        "id": "3a98dead"
      },
      "source": [
        "## 8. Prepare Test Data and Generate Predictions\n",
        "\n",
        "Now we prepare the actual test dataset based on the `SampleSubmission.csv` file. This involves:\n",
        "1.  Extracting the required future `date` and `user_id` from the `ID` column in the sample submission.\n",
        "2.  Merging the relevant aggregated daily `climate` data for those future dates.\n",
        "3.  Creating the same date-based `features` that the model was trained on.\n",
        "4.  Handling any potential missing climate data for future dates (e.g., by forward filling the last known values).\n",
        "5.  Using the trained `model` to predict `daily_kwh` for the test set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 16: Prepare Test Data and Predict (NO Rolling Features, with UserID)\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"--- Preparing Test Data (NO Rolling Features, with UserID) ---\")\n",
        "test_prep_start_time = time.time()\n",
        "\n",
        "# --- Check for required inputs ---\n",
        "if 'sample_sub' not in locals(): raise NameError(\"sample_sub not found.\")\n",
        "if 'daily_climate' not in locals(): raise NameError(\"daily_climate DataFrame not found.\")\n",
        "if 'df_train_full' not in locals(): raise NameError(\"df_train_full DataFrame not found.\")\n",
        "if 'create_date_features' not in locals(): raise NameError(\"create_date_features function not defined.\")\n",
        "# Ensure 'features' list from Cell 8 (NO ROLLING COLS) is used\n",
        "if 'features' not in locals(): raise NameError(\"features list not defined.\")\n",
        "if 'final_model' not in locals(): raise NameError(\"Trained 'final_model' not found.\")\n",
        "# Define lag/rolling lists based on FEATURES list for consistency\n",
        "lag_cols_created = [f for f in features if 'daily_kwh_lag_' in f]\n",
        "rolling_cols_created = [f for f in features if 'daily_kwh_roll_' in f] # Should be empty for this run\n",
        "print(f\"Expecting {len(lag_cols_created)} lag features based on 'features' list.\")\n",
        "print(f\"Expecting {len(rolling_cols_created)} rolling features based on 'features' list.\")\n",
        "\n",
        "\n",
        "# --- 1. Create Base Test Set Structure ---\n",
        "test_df_base = sample_sub[['ID']].copy()\n",
        "try:\n",
        "    test_df_base['date_str'] = test_df_base['ID'].str.split('_').str[0]\n",
        "    test_df_base['date'] = pd.to_datetime(test_df_base['date_str'], errors='coerce')\n",
        "    test_df_base['user_id'] = test_df_base['ID'].str.split('_', n=1).str[1]\n",
        "    test_df_base.dropna(subset=['date'], inplace=True)\n",
        "except Exception as e: raise ValueError(f\"Error parsing ID: {e}\")\n",
        "print(f\"[{time.time() - test_prep_start_time:.1f}s] Base test structure created.\")\n",
        "\n",
        "\n",
        "# --- 2. Merge Future Climate Data ---\n",
        "print(f\"[{time.time() - test_prep_start_time:.1f}s] Merging climate data...\")\n",
        "test_df = pd.merge(test_df_base, daily_climate, on='date', how='left')\n",
        "test_df.sort_values(by='date', inplace=True)\n",
        "test_df.ffill(inplace=True)\n",
        "test_df.bfill(inplace=True)\n",
        "climate_cols_in_test = [col for col in daily_climate.columns if col != 'date' and col in test_df.columns]\n",
        "print(f\"[{time.time() - test_prep_start_time:.1f}s] Climate data merged.\")\n",
        "\n",
        "\n",
        "# --- 3. Combine History for Feature Calculation ---\n",
        "print(f\"[{time.time() - test_prep_start_time:.1f}s] Combining history...\")\n",
        "# Determine max history needed based ONLY on lags (since rolling is off)\n",
        "max_lag = max((int(c.split('_')[-1]) for c in lag_cols_created), default=0)\n",
        "# max_roll_window = max((int(c.split('_')[-2]) for c in rolling_cols_created), default=0) # Not needed now\n",
        "history_needed_days = max_lag + 1 # Minimal buffer for lags\n",
        "\n",
        "min_test_date = test_df['date'].min()\n",
        "cutoff_date = min_test_date - pd.Timedelta(days=history_needed_days)\n",
        "print(f\"Max lag={max_lag}. Need history back to {cutoff_date.date()}\")\n",
        "\n",
        "cols_to_keep_hist = ['user_id', 'date', 'daily_kwh']\n",
        "historical_data = df_train_full[df_train_full['date'] < min_test_date][cols_to_keep_hist].copy()\n",
        "test_users = test_df['user_id'].unique()\n",
        "historical_data = historical_data[historical_data['user_id'].isin(test_users)]\n",
        "historical_data = historical_data[historical_data['date'] >= cutoff_date]\n",
        "\n",
        "future_data_cols = ['user_id', 'date'] + climate_cols_in_test\n",
        "future_data = test_df[future_data_cols].copy()\n",
        "future_data['daily_kwh'] = np.nan\n",
        "\n",
        "combined_df = pd.concat([historical_data, future_data], ignore_index=True)\n",
        "combined_df = combined_df.sort_values(by=['user_id', 'date'])\n",
        "combined_df.drop_duplicates(subset=['user_id', 'date'], keep='last', inplace=True)\n",
        "print(f\"Combined data shape: {combined_df.shape}\")\n",
        "\n",
        "\n",
        "# --- 4. Calculate Lag Features on Combined Data ---\n",
        "print(f\"[{time.time() - test_prep_start_time:.1f}s] Calculating lag features...\")\n",
        "if lag_cols_created:\n",
        "    target_col_for_lags = 'daily_kwh'\n",
        "    lags_to_create = [int(c.split('_')[-1]) for c in lag_cols_created]\n",
        "    if combined_df['user_id'].dtype.name != 'category':\n",
        "        print(\"Converting combined_df user_id to category...\")\n",
        "        combined_df['user_id'] = combined_df['user_id'].astype('category')\n",
        "    for lag in lags_to_create:\n",
        "        col_name = f\"daily_kwh_lag_{lag}\"\n",
        "        combined_df[col_name] = combined_df.groupby('user_id', observed=True)[target_col_for_lags].shift(lag)\n",
        "    print(\"Lag calculation finished.\")\n",
        "\n",
        "\n",
        "# --- 5. Calculate Rolling Window Features (SKIPPED) ---\n",
        "print(f\"\\n[{time.time() - test_prep_start_time:.1f}s] Skipping rolling window feature calculation.\")\n",
        "# (Keep rolling feature calculation code commented out or deleted)\n",
        "\n",
        "\n",
        "# --- 6. Create Date Features ---\n",
        "print(f\"\\n[{time.time() - test_prep_start_time:.1f}s] Creating date features...\")\n",
        "combined_df = create_date_features(combined_df, 'date')\n",
        "print(f\"Date features created.\")\n",
        "\n",
        "\n",
        "# --- 7. Filter Back to Required Test Dates ---\n",
        "print(f\"\\n[{time.time() - test_prep_start_time:.1f}s] Filtering back to required test dates...\")\n",
        "# Use the original test_df_base which has the 'ID' column\n",
        "test_df_final = pd.merge(test_df_base[['ID', 'user_id', 'date']], combined_df, on=['user_id', 'date'], how='left') #<<< test_df_final DEFINED HERE\n",
        "print(f\"Filtered back to test set shape: {test_df_final.shape}\")\n",
        "if len(test_df_final) != len(sample_sub): print(f\"WARNING: Row count mismatch! Got {len(test_df_final)}\")\n",
        "\n",
        "\n",
        "# --- 8. Final Checks and Feature Alignment ---\n",
        "print(f\"\\n[{time.time() - test_prep_start_time:.1f}s] Final checks and feature alignment...\")\n",
        "\n",
        "# Convert user_id to category in test_df_final\n",
        "if 'user_id' in test_df_final.columns:\n",
        "    if test_df_final['user_id'].dtype.name != 'category':\n",
        "        print(\"Converting user_id in test_df_final to category dtype...\")\n",
        "        test_df_final['user_id'] = test_df_final['user_id'].astype('category')\n",
        "else: print(\"Warning: user_id column not found in test_df_final.\")\n",
        "\n",
        "# Check feature availability using the 'features' list from Cell 8 (should NOT contain rolling)\n",
        "features_in_test = [f for f in features if f in test_df_final.columns]\n",
        "missing_features_in_test = [f for f in features if f not in test_df_final.columns]\n",
        "if missing_features_in_test:\n",
        "    print(f\"--- ERROR: Features defined in training missing from final test df: {missing_features_in_test} ---\")\n",
        "    print(f\"Columns available: {test_df_final.columns.tolist()}\")\n",
        "    raise ValueError(\"Mismatch between training 'features' list and test_df columns.\")\n",
        "\n",
        "# Handle NaNs (excluding categorical user_id)\n",
        "print(f\"Handling NaNs in final test features (filling with 0)...\")\n",
        "numeric_features_in_test = [f for f in features_in_test if f != 'user_id']\n",
        "test_df_final[numeric_features_in_test] = test_df_final[numeric_features_in_test].fillna(0)\n",
        "\n",
        "# Define X_test using ONLY the features available AND used in training\n",
        "X_test = test_df_final[features_in_test] # <<< X_test DEFINED HERE\n",
        "if X_test.isnull().any().any():\n",
        "    print(\"--- FATAL ERROR: NaNs found in X_test before prediction! ---\")\n",
        "    raise ValueError(\"NaNs detected in X_test.\")\n",
        "else:\n",
        "    print(\"[ OK ] Final test features (X_test) prepared. Shape: {X_test.shape}\")\n",
        "\n",
        "\n",
        "# --- 9. Predict on Test Set using FINAL MODEL ---\n",
        "print(f\"\\n[{time.time() - test_prep_start_time:.1f}s] Predicting using final_model...\")\n",
        "model_features = final_model.feature_name_ # Features model was trained on\n",
        "predict_features = [f for f in model_features if f in X_test.columns] # Ensure order and availability\n",
        "\n",
        "if len(predict_features) != len(model_features):\n",
        "    print(f\"Warning: Feature mismatch! Model trained on {len(model_features)}, predicting on {len(predict_features)}\")\n",
        "    X_test_reordered = X_test[predict_features]\n",
        "else:\n",
        "    X_test_reordered = X_test[model_features] # Use model's feature order\n",
        "\n",
        "test_predictions = final_model.predict(X_test_reordered)\n",
        "test_predictions_non_negative = np.maximum(0, test_predictions) # <<< test_predictions_non_negative DEFINED HERE\n",
        "if np.sum(test_predictions < 0) > 0: print(\"Note: Negative predictions clipped.\")\n",
        "\n",
        "print(\"Prediction on test set complete.\")\n",
        "test_prep_total_time = time.time() - test_prep_start_time\n",
        "print(f\"\\n[{test_prep_total_time:.1f}s] Cell 16 processing finished.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yCczLNpMc3R-",
        "outputId": "00bdffd6-2b9c-409d-bbdf-4af8c326ea22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "yCczLNpMc3R-",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Preparing Test Data (NO Rolling Features, with UserID) ---\n",
            "Expecting 6 lag features based on 'features' list.\n",
            "Expecting 0 rolling features based on 'features' list.\n",
            "[0.0s] Base test structure created.\n",
            "[0.0s] Merging climate data...\n",
            "[0.1s] Climate data merged.\n",
            "[0.1s] Combining history...\n",
            "Max lag=28. Need history back to 2024-08-26\n",
            "Combined data shape: (11640, 7)\n",
            "[0.2s] Calculating lag features...\n",
            "Converting combined_df user_id to category...\n",
            "Lag calculation finished.\n",
            "\n",
            "[0.2s] Skipping rolling window feature calculation.\n",
            "\n",
            "[0.2s] Creating date features...\n",
            "Date features created.\n",
            "\n",
            "[0.2s] Filtering back to required test dates...\n",
            "Filtered back to test set shape: (6014, 22)\n",
            "\n",
            "[0.2s] Final checks and feature alignment...\n",
            "Converting user_id in test_df_final to category dtype...\n",
            "Handling NaNs in final test features (filling with 0)...\n",
            "[ OK ] Final test features (X_test) prepared. Shape: {X_test.shape}\n",
            "\n",
            "[0.2s] Predicting using final_model...\n",
            "Note: Negative predictions clipped.\n",
            "Prediction on test set complete.\n",
            "\n",
            "[0.7s] Cell 16 processing finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Generate submission file"
      ],
      "metadata": {
        "id": "1orzcQo3c9LU"
      },
      "id": "1orzcQo3c9LU"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "15ab428b",
      "metadata": {
        "id": "15ab428b",
        "outputId": "0243196a-008c-4236-ea13-0feaf91d87ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating submission file...\n",
            "\n",
            "--- Submission File Head ---\n",
            "                                          ID       kwh\n",
            "0  2024-09-24_consumer_device_12_data_user_1  0.158338\n",
            "1  2024-09-25_consumer_device_12_data_user_1  0.076580\n",
            "2  2024-09-26_consumer_device_12_data_user_1  0.026149\n",
            "3  2024-09-27_consumer_device_12_data_user_1  0.017782\n",
            "4  2024-09-28_consumer_device_12_data_user_1  0.013799\n",
            "\n",
            "Submission file saved successfully as: submission_v7_optuna_tuned.csv\n"
          ]
        }
      ],
      "source": [
        "# Cell 18: Generate Submission File (v7 - Optuna Tuned)\n",
        "\n",
        "print(\"Generating submission file...\")\n",
        "\n",
        "submission_df = pd.DataFrame({'ID': test_df_final['ID'], 'kwh': test_predictions_non_negative})\n",
        "\n",
        "# --- CHANGED FILENAME ---\n",
        "submission_filename = 'submission_v7_optuna_tuned.csv'\n",
        "\n",
        "submission_df.to_csv(submission_filename, index=False)\n",
        "\n",
        "print(\"\\n--- Submission File Head ---\")\n",
        "print(submission_df.head())\n",
        "print(f\"\\nSubmission file saved successfully as: {submission_filename}\")\n",
        "# (Keep row count check)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cc93c28",
      "metadata": {
        "id": "5cc93c28"
      },
      "source": [
        "--- End of Baseline Notebook ---\n",
        "\n",
        "Next steps:\n",
        "- Submit the generated CSV to Zindi.\n",
        "- Analyze the results (local RMSE vs Zindi score).\n",
        "- Improve the model by:\n",
        "    - Adding more features (lags, rolling windows, user features).\n",
        "    - Tuning hyperparameters (e.g., using Optuna).\n",
        "    - Trying different models (XGBoost, CatBoost).\n",
        "    - Implementing more robust validation (Time Series Cross-Validation).\n",
        "    - Ensembling models."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "zindi_mhp_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "60c2519fc1d049ee821397dd19805236": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_78ec9de80b194802b77e9de64d56973f",
              "IPY_MODEL_82fd74673b3a4a8080e00a95759bb1a6",
              "IPY_MODEL_009f97a1d6564b18ad848adb72c7b3bd"
            ],
            "layout": "IPY_MODEL_122a1282f64e431294c4cacdd73d33cd"
          }
        },
        "78ec9de80b194802b77e9de64d56973f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_188d84ce1a04404c82f9df1c2f9dec45",
            "placeholder": "​",
            "style": "IPY_MODEL_e3066e11c14d40f387293471394c1c4a",
            "value": "Best trial: 19. Best value: 2.13136: 100%"
          }
        },
        "82fd74673b3a4a8080e00a95759bb1a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6a59c07a4d44ca5b12a25ae256fb7ed",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_876d51e2a5d340c0a8d7421f334a0ee1",
            "value": 20
          }
        },
        "009f97a1d6564b18ad848adb72c7b3bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8abe1bc4c8f4bc3baf69943021cfbcb",
            "placeholder": "​",
            "style": "IPY_MODEL_96f8a848863942c6a3e3fa1b63534976",
            "value": " 20/20 [20:34&lt;00:00, 82.81s/it]"
          }
        },
        "122a1282f64e431294c4cacdd73d33cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "188d84ce1a04404c82f9df1c2f9dec45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3066e11c14d40f387293471394c1c4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6a59c07a4d44ca5b12a25ae256fb7ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "876d51e2a5d340c0a8d7421f334a0ee1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8abe1bc4c8f4bc3baf69943021cfbcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96f8a848863942c6a3e3fa1b63534976": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}