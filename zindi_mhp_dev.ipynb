{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "562fd2ac",
   "metadata": {},
   "source": [
    "# Zindi Micro-Hydropower Energy Load Prediction\n",
    "\n",
    "This notebook aims to predict daily energy consumption (kWh) per data user for Micro-Hydropower Plants (MHPs) in Kalam, Pakistan. We will use MHP sensor data and climate indicators to build a predictive model.\n",
    "\n",
    "**Objective:** Forecast total daily kWh per user for one month into the future.\n",
    "**Metric:** Root Mean Squared Error (RMSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f49b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and settings configured.\n",
      "Data directory set to: data\n",
      "Available plotting styles: ['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'petroff10', 'seaborn-v0_8', 'seaborn-v0_8-bright', 'seaborn-v0_8-colorblind', 'seaborn-v0_8-dark', 'seaborn-v0_8-dark-palette', 'seaborn-v0_8-darkgrid', 'seaborn-v0_8-deep', 'seaborn-v0_8-muted', 'seaborn-v0_8-notebook', 'seaborn-v0_8-paper', 'seaborn-v0_8-pastel', 'seaborn-v0_8-poster', 'seaborn-v0_8-talk', 'seaborn-v0_8-ticks', 'seaborn-v0_8-white', 'seaborn-v0_8-whitegrid', 'tableau-colorblind10']\n"
     ]
    }
   ],
   "source": [
    "# Basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math # For sqrt\n",
    "import os\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Settings\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "DATA_DIR = 'data' # Set the path to your data directory\n",
    "\n",
    "# Ensure plots are displayed inline and set a style\n",
    "%matplotlib inline\n",
    "# Use a style that's likely available - adjust if needed\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "# If the above fails, try: plt.style.use('seaborn-darkgrid') or plt.style.use('ggplot')\n",
    "\n",
    "print(\"Libraries imported and settings configured.\")\n",
    "print(f\"Data directory set to: {DATA_DIR}\")\n",
    "print(f\"Available plotting styles: {plt.style.available}\") # Optional: see available styles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02b6671",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "Load the datasets:\n",
    "1.  **MHP Data:** `Data.csv` contains the 5-minute interval sensor readings (voltage, current, kWh, etc.). We anticipate the timestamp column is named `date_time`.\n",
    "2.  **Climate Data:** `Kalam Climate Data.xlsx` contains climate indicators (temperature, precipitation, etc.). This is an Excel file.\n",
    "3.  **Sample Submission:** `SampleSubmission.csv` defines the required prediction format and IDs for the test set.\n",
    "\n",
    "*Note: Reading Excel files requires the `openpyxl` library. Install it if needed (`pip install openpyxl`). Reading the MHP CSV might require `engine='python'` if the default C engine fails.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46194803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Attempting to load MHP data from: data\\Data.csv\n",
      "Loading only columns: ['date_time', 'Source', 'kwh']\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "\n",
    "try:\n",
    "    # --- Load MHP Data (CSV) - OPTIMIZED ---\n",
    "    mhp_filename = 'Data.csv'\n",
    "    mhp_path = os.path.join(DATA_DIR, mhp_filename)\n",
    "    print(f\"Attempting to load MHP data from: {mhp_path}\")\n",
    "\n",
    "    # ---> Specify ONLY the columns needed for aggregation <---\n",
    "    mhp_cols_to_load = ['date_time', 'Source', 'kwh']\n",
    "    print(f\"Loading only columns: {mhp_cols_to_load}\")\n",
    "\n",
    "    mhp_data_raw = pd.read_csv(\n",
    "        mhp_path,\n",
    "        usecols=mhp_cols_to_load,       # <--- ADDED usecols\n",
    "        parse_dates=['date_time'],      # Still parse the date column\n",
    "        engine='python'                 # Keep engine='python' as C engine failed before\n",
    "    )\n",
    "    mhp_data_raw.rename(columns={'date_time': 'timestamp', 'Source': 'user_id'}, inplace=True) # Rename here\n",
    "    print(f\"Successfully loaded columns from: {mhp_filename}\")\n",
    "    \n",
    "\n",
    "    # --- Load Climate Data (Excel) ---\n",
    "    climate_filename = 'Kalam Climate Data.xlsx'\n",
    "    climate_path = os.path.join(DATA_DIR, climate_filename)\n",
    "    print(f\"Attempting to load climate data from: {climate_path}\")\n",
    "    # Use pd.read_excel for .xlsx files. Ensure openpyxl is installed.\n",
    "    # Confirm the timestamp column name in the Excel file. Use that name in parse_dates.\n",
    "    # Assuming it's 'timestamp' based on previous context, adjust if needed.\n",
    "    climate_data_raw = pd.read_excel(climate_path, parse_dates=['Date_Time'])\n",
    "    # If the Excel timestamp column was different (e.g., 'date_time'), uncomment and adjust the lines below:\n",
    "    # climate_data_raw = pd.read_excel(climate_path, parse_dates=['date_time'])\n",
    "    climate_data_raw.rename(columns={'Date_Time': 'timestamp'}, inplace=True)\n",
    "    print(f\"Successfully loaded: {climate_filename}\")\n",
    "\n",
    "    # --- Load Sample Submission (CSV) ---\n",
    "    sample_sub_filename = 'SampleSubmission.csv'\n",
    "    sample_sub_path = os.path.join(DATA_DIR, sample_sub_filename)\n",
    "    print(f\"Attempting to load sample submission from: {sample_sub_path}\")\n",
    "    sample_sub = pd.read_csv(sample_sub_path)\n",
    "    print(f\"Successfully loaded: {sample_sub_filename}\")\n",
    "\n",
    "    print(\"\\nAll data loaded successfully.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n--- ERROR: File Not Found ---\")\n",
    "    print(f\"{e}\")\n",
    "    print(f\"Please ensure 'Data.csv', 'Kalam Climate Data.xlsx', and 'SampleSubmission.csv' are directly in the '{DATA_DIR}' directory.\")\n",
    "    raise\n",
    "except ImportError as e:\n",
    "     print(f\"\\n--- ERROR: Missing Library ---\")\n",
    "     print(f\"{e}\")\n",
    "     print(\"Failed loading Excel file. You might need to install the required engine.\")\n",
    "     print(\"In your terminal (with venv active), run: pip install openpyxl\")\n",
    "     raise\n",
    "except ValueError as e:\n",
    "    print(f\"\\n--- ERROR: Value Error during loading/parsing ---\")\n",
    "    print(f\"{e}\")\n",
    "    print(\"This might be due to an incorrect column name in 'parse_dates'. Verify column names in the source files.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "     print(f\"\\n--- ERROR: An unexpected error occurred during data loading ---\")\n",
    "     print(f\"{e}\")\n",
    "     raise\n",
    "\n",
    "# --- Display Info (Optional but recommended) ---\n",
    "print(\"\\n--- MHP Data Info ---\")\n",
    "mhp_data_raw.info()\n",
    "print(\"\\n--- Climate Data Info ---\")\n",
    "climate_data_raw.info()\n",
    "print(\"\\n--- Sample Submission Head ---\")\n",
    "print(sample_sub.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2115812",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Aggregation\n",
    "\n",
    "The MHP data is recorded at 5-minute intervals, but the prediction target is daily kWh per user. Climate data might also be at a finer granularity than daily.\n",
    "\n",
    "We need to:\n",
    "1.  Aggregate the MHP `kwh` readings to get the total daily sum for each `user_id`.\n",
    "2.  Aggregate the climate data to daily statistics (e.g., mean temperature, total precipitation).\n",
    "3.  Merge the aggregated daily MHP data with the aggregated daily climate data.\n",
    "4.  Handle any missing values that might arise from the merge or exist in the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2981e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Aggregating data to daily level...\")\n",
    "\n",
    "# --- MHP Data Aggregation ---\n",
    "# Ensure the timestamp column exists after loading and renaming\n",
    "if 'timestamp' not in mhp_data_raw.columns:\n",
    "    raise KeyError(\"Column 'timestamp' not found in mhp_data_raw. Check loading and renaming step.\")\n",
    "\n",
    "mhp_data = mhp_data_raw.copy()\n",
    "# Extract date from the timestamp\n",
    "mhp_data['date'] = mhp_data['timestamp'].dt.date\n",
    "# The user identifier column seems to be 'Source' based on previous inspection, rename it to user_id for clarity\n",
    "# If the user identifier is different (e.g., 'ID'), change 'Source' below\n",
    "if 'Source' in mhp_data.columns:\n",
    "    mhp_data.rename(columns={'Source': 'user_id'}, inplace=True)\n",
    "elif 'ID' in mhp_data.columns:\n",
    "     mhp_data.rename(columns={'ID': 'user_id'}, inplace=True)\n",
    "else:\n",
    "    raise KeyError(\"Could not find user identifier column ('Source' or 'ID') in mhp_data.\")\n",
    "\n",
    "# Group by the user ID and the date, then sum the kWh for that day\n",
    "daily_kwh = mhp_data.groupby(['user_id', 'date'])['kwh'].sum().reset_index()\n",
    "daily_kwh.rename(columns={'kwh': 'daily_kwh'}, inplace=True)\n",
    "# Convert date back to datetime object for merging and feature engineering\n",
    "daily_kwh['date'] = pd.to_datetime(daily_kwh['date'])\n",
    "print(f\"Aggregated MHP data shape: {daily_kwh.shape}\")\n",
    "print(\"Aggregated Daily kWh per User Head:\")\n",
    "print(daily_kwh.head())\n",
    "\n",
    "# --- Climate Data Aggregation ---\n",
    "# Ensure the timestamp column exists\n",
    "if 'timestamp' not in climate_data_raw.columns:\n",
    "    raise KeyError(\"Column 'timestamp' not found in climate_data_raw. Check loading and renaming step.\")\n",
    "\n",
    "climate_data = climate_data_raw.copy()\n",
    "# Extract date from the timestamp\n",
    "climate_data['date'] = climate_data['timestamp'].dt.date\n",
    "# Aggregate climate features to daily level\n",
    "daily_climate = climate_data.groupby('date').agg(\n",
    "    temp_mean=('temperature', 'mean'),\n",
    "    dew_point_mean=('dew_point', 'mean'),\n",
    "    wind_speed_mean=('wind_speed', 'mean'),\n",
    "    precip_sum=('precipitation', 'sum')\n",
    "    # Add other aggregations if needed (min/max temp etc.)\n",
    ").reset_index()\n",
    "# Convert date back to datetime object\n",
    "daily_climate['date'] = pd.to_datetime(daily_climate['date'])\n",
    "print(f\"\\nAggregated climate data shape: {daily_climate.shape}\")\n",
    "print(\"Aggregated Daily Climate Head:\")\n",
    "print(daily_climate.head())\n",
    "\n",
    "\n",
    "# --- Merge Aggregated Data ---\n",
    "print(\"\\nMerging aggregated daily MHP and climate data...\")\n",
    "df_train_full = pd.merge(daily_kwh, daily_climate, on='date', how='left')\n",
    "print(f\"Initial merged shape: {df_train_full.shape}\")\n",
    "\n",
    "# --- Handle Missing Values (Simple Strategy) ---\n",
    "# Check NaNs introduced by merge or existing in climate data\n",
    "print(f\"\\nNaNs before fill:\\n{df_train_full.isnull().sum()}\")\n",
    "# Use forward fill first to propagate last known values, then back fill for any remaining at the start\n",
    "df_train_full.sort_values(by=['user_id', 'date'], inplace=True) # Sort before filling\n",
    "df_train_full.fillna(method='ffill', inplace=True)\n",
    "df_train_full.fillna(method='bfill', inplace=True)\n",
    "print(f\"\\nNaNs after fill: {df_train_full.isnull().sum().sum()}\") # Should ideally be 0\n",
    "\n",
    "print(\"\\n--- Merged & Cleaned Training Data Head ---\")\n",
    "print(df_train_full.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729be8bf",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Create features based on the date and potentially other aspects of the data. For this baseline, we will focus on date-based features. More advanced features (lags, rolling windows, user-specific stats) can be added later for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f9eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating date-based features...\")\n",
    "\n",
    "def create_date_features(df, date_col='date'):\n",
    "    \"\"\"Creates time series features from a date column.\"\"\"\n",
    "    df = df.copy() # Avoid SettingWithCopyWarning\n",
    "    df[date_col] = pd.to_datetime(df[date_col]) # Ensure it's datetime\n",
    "    df['year'] = df[date_col].dt.year\n",
    "    df['month'] = df[date_col].dt.month\n",
    "    df['day'] = df[date_col].dt.day\n",
    "    df['dayofweek'] = df[date_col].dt.dayofweek # Monday=0, Sunday=6\n",
    "    df['dayofyear'] = df[date_col].dt.dayofyear\n",
    "    # Use .dt.isocalendar().week for ISO standard week number\n",
    "    df['weekofyear'] = df[date_col].dt.isocalendar().week.astype(int)\n",
    "    df['quarter'] = df[date_col].dt.quarter\n",
    "    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)\n",
    "    # Example: Add sine/cosine features for month (captures cyclicity)\n",
    "    # df['month_sin'] = np.sin(2 * np.pi * df['month']/12.0)\n",
    "    # df['month_cos'] = np.cos(2 * np.pi * df['month']/12.0)\n",
    "    return df\n",
    "\n",
    "# Apply feature creation function to the merged data\n",
    "df_train_full = create_date_features(df_train_full, 'date')\n",
    "\n",
    "# --- Define Features and Target ---\n",
    "# List of columns to be used as input features for the model\n",
    "features = [\n",
    "    # Date features\n",
    "    'year', 'month', 'day', 'dayofweek', 'dayofyear', 'weekofyear', 'quarter', 'is_weekend',\n",
    "    # Climate features\n",
    "    'temp_mean', 'dew_point_mean', 'wind_speed_mean', 'precip_sum'\n",
    "    # Add more features here later:\n",
    "    # e.g., lag features, rolling window features, user id encodings\n",
    "]\n",
    "# The column we want to predict\n",
    "target = 'daily_kwh'\n",
    "\n",
    "# Make sure all feature columns are numeric for the model (important for some models)\n",
    "print(\"\\nChecking feature data types...\")\n",
    "for col in features:\n",
    "    if col not in df_train_full.columns:\n",
    "        raise ValueError(f\"Feature '{col}' not found in dataframe columns: {df_train_full.columns}\")\n",
    "    if not pd.api.types.is_numeric_dtype(df_train_full[col]):\n",
    "        print(f\"Warning: Feature '{col}' is not numeric ({df_train_full[col].dtype}). Attempting conversion.\")\n",
    "        df_train_full[col] = pd.to_numeric(df_train_full[col], errors='coerce')\n",
    "        # Handle potential NaNs created by coerce if conversion fails\n",
    "        if df_train_full[col].isnull().any():\n",
    "             print(f\"NaNs created in {col} after conversion, filling with 0\")\n",
    "             df_train_full[col].fillna(0, inplace=True) # Or use mean/median\n",
    "\n",
    "print(\"Features defined and checked.\")\n",
    "print(f\"List of features ({len(features)}): {features}\")\n",
    "\n",
    "print(\"\\n--- Training Data with Features Head ---\")\n",
    "print(df_train_full.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a487309a",
   "metadata": {},
   "source": [
    "## 5. Train/Validation Split (Time-Based)\n",
    "\n",
    "For time series forecasting, it's crucial to validate the model on data that comes *after* the training data. We will split the data chronologically, using the most recent period for validation. Shuffling should **not** be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd68efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Splitting data into train and validation sets based on time...\")\n",
    "\n",
    "# Ensure data is sorted chronologically overall (important for time-based split)\n",
    "# Sorting by user then date is already done, but an overall sort helps define the cutoff\n",
    "df_train_full = df_train_full.sort_values(by='date')\n",
    "\n",
    "# Define validation period (e.g., last 30 days of available data)\n",
    "# Find the overall max date in the dataset\n",
    "max_date = df_train_full['date'].max()\n",
    "# Calculate the start date for the validation set\n",
    "# Ensure there are enough days for a meaningful validation set\n",
    "validation_days = 30\n",
    "validation_start_date = max_date - pd.Timedelta(days=validation_days - 1) # -1 because we include the start date\n",
    "\n",
    "print(f\"Overall data range: {df_train_full['date'].min()} to {max_date}\")\n",
    "print(f\"Using data from {validation_start_date} onwards for validation ({validation_days} days).\")\n",
    "\n",
    "# Split the data\n",
    "train_data = df_train_full[df_train_full['date'] < validation_start_date].copy()\n",
    "val_data = df_train_full[df_train_full['date'] >= validation_start_date].copy()\n",
    "\n",
    "# Ensure there's data in both sets\n",
    "if train_data.empty or val_data.empty:\n",
    "     raise ValueError(\"Train or validation set is empty. Check data range and validation_days.\")\n",
    "\n",
    "print(f\"\\nTraining data shape: {train_data.shape}\")\n",
    "print(f\"Validation data shape: {val_data.shape}\")\n",
    "print(f\"Training data period: {train_data['date'].min().date()} to {train_data['date'].max().date()}\")\n",
    "print(f\"Validation data period: {val_data['date'].min().date()} to {val_data['date'].max().date()}\")\n",
    "\n",
    "# Separate features (X) and target (y) for train and validation sets\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[target]\n",
    "X_val = val_data[features]\n",
    "y_val = val_data[target]\n",
    "\n",
    "print(\"\\nTrain/Validation split complete.\")\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0e2e18",
   "metadata": {},
   "source": [
    "## 6. Model Training (LightGBM Baseline)\n",
    "\n",
    "We will use LightGBM, a gradient boosting framework known for its speed and efficiency, to train a baseline model. We'll use early stopping based on the validation set performance (RMSE) to prevent overfitting and find a reasonable number of boosting rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training LightGBM model...\")\n",
    "\n",
    "# Define LightGBM parameters\n",
    "lgb_params = {\n",
    "    'objective': 'regression_l1', # Use 'regression_l1' (MAE) for robustness to outliers, or 'regression_l2' (RMSE)\n",
    "    'metric': 'rmse',             # Evaluation metric used for early stopping\n",
    "    'n_estimators': 2000,         # Max number of trees; early stopping will find the optimal number\n",
    "    'learning_rate': 0.03,        # Step size shrinkage\n",
    "    'feature_fraction': 0.8,      # Fraction of features to consider per tree\n",
    "    'bagging_fraction': 0.8,      # Fraction of data to sample per tree (requires bagging_freq > 0)\n",
    "    'bagging_freq': 1,            # Perform bagging on every iteration\n",
    "    'lambda_l1': 0.1,             # L1 regularization\n",
    "    'lambda_l2': 0.1,             # L2 regularization\n",
    "    'num_leaves': 31,             # Max number of leaves in one tree (controls complexity)\n",
    "    'verbose': -1,                # Suppress verbose logging during training\n",
    "    'n_jobs': -1,                 # Use all available CPU cores\n",
    "    'seed': SEED,                 # Random seed for reproducibility\n",
    "    'boosting_type': 'gbdt',      # Standard gradient boosted decision trees\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "model = lgb.LGBMRegressor(**lgb_params)\n",
    "\n",
    "# Train the model with early stopping\n",
    "print(\"Starting model fitting...\")\n",
    "model.fit(X_train, y_train,\n",
    "          eval_set=[(X_val, y_val)],          # Data to evaluate on during training\n",
    "          eval_metric='rmse',                # Metric for evaluation\n",
    "          callbacks=[lgb.early_stopping(stopping_rounds=100, # Stop if metric doesn't improve for 100 rounds\n",
    "                                        verbose=100)])      # Print progress every 100 rounds\n",
    "\n",
    "print(\"\\nModel training complete.\")\n",
    "print(f\"Best iteration found by early stopping: {model.best_iteration_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfaa893",
   "metadata": {},
   "source": [
    "## 7. Local Validation (Calculate RMSE)\n",
    "\n",
    "Evaluate the trained model's performance on the unseen validation set. This gives us an estimate of how well the model might perform on the actual test data on Zindi. We will calculate the Root Mean Squared Error (RMSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6fbcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating model on the validation set...\")\n",
    "\n",
    "# Predict on the validation set features\n",
    "val_preds = model.predict(X_val)\n",
    "\n",
    "# Optional: Ensure no negative predictions (kWh cannot be negative)\n",
    "val_preds_non_negative = np.maximum(0, val_preds)\n",
    "negative_preds_count = np.sum(val_preds < 0)\n",
    "if negative_preds_count > 0:\n",
    "    print(f\"Warning: {negative_preds_count} negative predictions were clipped to 0.\")\n",
    "\n",
    "# --- Calculate RMSE using the non-negative predictions ---\n",
    "rmse = math.sqrt(mean_squared_error(y_val, val_preds_non_negative))\n",
    "print(f\"\\nValidation RMSE: {rmse:.4f}\")\n",
    "\n",
    "# --- Feature Importance Plot ---\n",
    "# Plotting helps understand which features the model found most useful\n",
    "print(\"\\nPlotting feature importance...\")\n",
    "try:\n",
    "    lgb.plot_importance(model, figsize=(10, max(6, len(features)//2)), max_num_features=len(features))\n",
    "    plt.title(f'LightGBM Feature Importance (Validation RMSE: {rmse:.4f})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as plot_err:\n",
    "    print(f\"Could not plot feature importance: {plot_err}\")\n",
    "\n",
    "# Optional: Analyze Residuals\n",
    "# residuals = y_val - val_preds_non_negative\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.histplot(residuals, bins=50, kde=True)\n",
    "# plt.title('Distribution of Residuals (Actual - Predicted)')\n",
    "# plt.xlabel('Error (kWh)')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(val_preds_non_negative, residuals, alpha=0.5)\n",
    "# plt.title('Residuals vs. Predicted Values')\n",
    "# plt.xlabel('Predicted kWh')\n",
    "# plt.ylabel('Residual (Actual - Predicted)')\n",
    "# plt.axhline(0, color='red', linestyle='--')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a98dead",
   "metadata": {},
   "source": [
    "## 8. Prepare Test Data and Generate Predictions\n",
    "\n",
    "Now we prepare the actual test dataset based on the `SampleSubmission.csv` file. This involves:\n",
    "1.  Extracting the required future `date` and `user_id` from the `ID` column in the sample submission.\n",
    "2.  Merging the relevant aggregated daily `climate` data for those future dates.\n",
    "3.  Creating the same date-based `features` that the model was trained on.\n",
    "4.  Handling any potential missing climate data for future dates (e.g., by forward filling the last known values).\n",
    "5.  Using the trained `model` to predict `daily_kwh` for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ab428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing test data for final prediction...\")\n",
    "\n",
    "# --- Create Test Set Structure from Sample Submission ---\n",
    "if 'ID' not in sample_sub.columns:\n",
    "    raise KeyError(\"Column 'ID' not found in SampleSubmission.csv.\")\n",
    "\n",
    "test_df = sample_sub[['ID']].copy()\n",
    "# Extract date and user_id from the submission ID string\n",
    "try:\n",
    "    test_df['date_str'] = test_df['ID'].str.split('_').str[0]\n",
    "    test_df['date'] = pd.to_datetime(test_df['date_str'], errors='coerce')\n",
    "    # Reconstruct user_id (everything after the date and the first underscore)\n",
    "    test_df['user_id'] = test_df['ID'].str.split('_', n=1).str[1]\n",
    "except Exception as e:\n",
    "    print(f\"Error parsing ID column in sample submission: {e}\")\n",
    "    raise\n",
    "\n",
    "# Drop rows where date parsing failed, if any\n",
    "invalid_dates = test_df['date'].isnull().sum()\n",
    "if invalid_dates > 0:\n",
    "    print(f\"Warning: Removed {invalid_dates} rows from test set due to invalid date format in ID.\")\n",
    "    test_df.dropna(subset=['date'], inplace=True)\n",
    "\n",
    "print(f\"Test set initial structure created. Shape: {test_df.shape}\")\n",
    "print(\"Test DataFrame Head (with parsed date/user):\")\n",
    "print(test_df.head())\n",
    "\n",
    "# --- Merge Future Climate Data ---\n",
    "# Use the aggregated daily climate data prepared earlier\n",
    "test_df = pd.merge(test_df, daily_climate, on='date', how='left')\n",
    "print(f\"Test set shape after merging climate data: {test_df.shape}\")\n",
    "\n",
    "# --- Handle Missing Climate Data for Future Dates ---\n",
    "# Important: Future dates might not have climate data in our historical set.\n",
    "# Sort by date first to ensure correct forward filling\n",
    "test_df.sort_values(by='date', inplace=True)\n",
    "# Forward fill using the last known climate values\n",
    "test_df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Check if any NaNs remain (e.g., if test dates are *before* the first climate date)\n",
    "remaining_nans = test_df[daily_climate.columns.difference(['date'])].isnull().sum()\n",
    "if remaining_nans.sum() > 0:\n",
    "     print(f\"\\nWarning: NaNs remain in test climate data after forward fill:\")\n",
    "     print(remaining_nans[remaining_nans > 0])\n",
    "     print(\"This might happen if test dates are before the start of climate data.\")\n",
    "     print(\"Filling remaining NaNs with 0 - consider a more robust strategy (e.g., overall mean/median) if this occurs.\")\n",
    "     test_df.fillna(0, inplace=True) # Simple strategy for remaining NaNs\n",
    "\n",
    "# --- Create Features for Test Set ---\n",
    "print(\"\\nCreating features for the test set...\")\n",
    "test_df = create_date_features(test_df, 'date')\n",
    "\n",
    "# --- Align Test Columns with Training Features ---\n",
    "# Ensure test set has exactly the same feature columns as the training set, in the same order\n",
    "try:\n",
    "    X_test = test_df[features]\n",
    "except KeyError as e:\n",
    "    print(f\"Error aligning test features: Missing column {e}\")\n",
    "    print(f\"Columns available in test_df: {test_df.columns.tolist()}\")\n",
    "    print(f\"Features expected by model: {features}\")\n",
    "    raise\n",
    "\n",
    "# Check for NaNs in the final test features (should not happen after fillna steps)\n",
    "if X_test.isnull().any().any():\n",
    "    print(\"\\n--- FATAL ERROR: NaNs found in final test features before prediction! ---\")\n",
    "    print(X_test.isnull().sum())\n",
    "    raise ValueError(\"NaNs detected in test features. Check data preparation and fillna steps.\")\n",
    "else:\n",
    "    print(\"\\nTest features successfully prepared and checked for NaNs.\")\n",
    "\n",
    "print(f\"Final test data shape for prediction (X_test): {X_test.shape}\")\n",
    "print(\"Test Data Head (Ready for Prediction):\")\n",
    "print(X_test.head())\n",
    "\n",
    "# --- Predict on Test Set ---\n",
    "print(\"\\nPredicting on the prepared test set...\")\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Ensure no negative predictions\n",
    "test_predictions_non_negative = np.maximum(0, test_predictions)\n",
    "test_neg_count = np.sum(test_predictions < 0)\n",
    "if test_neg_count > 0:\n",
    "    print(f\"Note: {test_neg_count} negative predictions in test set were clipped to 0.\")\n",
    "\n",
    "print(\"Prediction on test set complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b01330",
   "metadata": {},
   "source": [
    "## 9. Generate Submission File\n",
    "\n",
    "Create the final submission file in the format required by Zindi: a CSV file with two columns, `ID` and `kwh`. The `ID` column must match the `SampleSubmission.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dba38c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating submission file...\")\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission_df = pd.DataFrame({'ID': test_df['ID'], 'kwh': test_predictions_non_negative})\n",
    "\n",
    "# Define the submission filename\n",
    "submission_filename = 'submission_baseline_lgb_v1.csv' # Increment version number as you improve\n",
    "\n",
    "# Save the submission file\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(\"\\n--- Submission File Head ---\")\n",
    "print(submission_df.head())\n",
    "print(f\"\\nSubmission file saved successfully as: {submission_filename}\")\n",
    "print(f\"File shape: {submission_df.shape}\")\n",
    "\n",
    "# Optional: Check if the number of rows matches the sample submission\n",
    "if len(submission_df) == len(sample_sub):\n",
    "    print(\"Number of rows matches SampleSubmission.csv.\")\n",
    "else:\n",
    "    print(f\"Warning: Row count mismatch! Submission has {len(submission_df)} rows, SampleSub has {len(sample_sub)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc93c28",
   "metadata": {},
   "source": [
    "--- End of Baseline Notebook ---\n",
    "\n",
    "Next steps:\n",
    "- Submit the generated CSV to Zindi.\n",
    "- Analyze the results (local RMSE vs Zindi score).\n",
    "- Improve the model by:\n",
    "    - Adding more features (lags, rolling windows, user features).\n",
    "    - Tuning hyperparameters (e.g., using Optuna).\n",
    "    - Trying different models (XGBoost, CatBoost).\n",
    "    - Implementing more robust validation (Time Series Cross-Validation).\n",
    "    - Ensembling models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zindi_mhp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
